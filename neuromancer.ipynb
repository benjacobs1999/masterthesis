{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import neuromancer as nm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.autograd.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"QP_data/original/random_simple_dataset_var100_ineq50_eq50_ex10000\"\n",
    "\n",
    "with open(filepath, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, eq_cm, ineq_cm, eq_rhs, ineq_rhs):\n",
    "        self.x = x\n",
    "        self.eq_cm = eq_cm \n",
    "        self.ineq_cm = ineq_cm\n",
    "        self.eq_rhs = eq_rhs\n",
    "        self.ineq_rhs = ineq_rhs\n",
    "        self._index = 0  # Internal index for tracking iteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple of input and target for the given index\n",
    "        #! Change per data set.\n",
    "        # return self.x[idx], self.eq_cm[idx], self.ineq_cm[idx], self.eq_rhs[idx], self.ineq_rhs[idx]\n",
    "        return self.x[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eq_cm.shape: torch.Size([50, 100])\n",
      "ineq_cm.shape: torch.Size([50, 100])\n",
      "eq_rhs.shape: torch.Size([10000, 50])\n",
      "ineq_rhs.shape: torch.Size([50])\n",
      "Q.shape: torch.Size([100, 100])\n",
      "p.shape: torch.Size([100])\n",
      "torch.Size([8000, 50])\n",
      "8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjacobs/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:169: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([200, 50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(left, right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: -36.84498300780661\n",
      "epoch: 1  train_loss: -50.47986237821413\n",
      "epoch: 2  train_loss: -50.67859629690764\n",
      "epoch: 3  train_loss: -50.68386467574614\n",
      "epoch: 4  train_loss: -50.69908108232159\n",
      "epoch: 5  train_loss: -50.70000986354354\n",
      "epoch: 6  train_loss: -50.69434455754058\n",
      "epoch: 7  train_loss: -50.72722819633897\n",
      "epoch: 8  train_loss: -50.740797609557404\n",
      "epoch: 9  train_loss: -50.722500858065956\n",
      "epoch: 10  train_loss: -50.74292066495661\n",
      "epoch: 11  train_loss: -50.737590612690894\n",
      "epoch: 12  train_loss: -50.78030461830026\n",
      "epoch: 13  train_loss: -50.76081135564972\n",
      "epoch: 14  train_loss: -50.750392788929425\n",
      "epoch: 15  train_loss: -50.79590942025992\n",
      "epoch: 16  train_loss: -50.78301074716298\n",
      "epoch: 17  train_loss: -50.774992834006625\n",
      "epoch: 18  train_loss: -50.80526810826696\n",
      "epoch: 19  train_loss: -50.78743292209125\n",
      "epoch: 20  train_loss: -50.81181337292083\n",
      "epoch: 21  train_loss: -50.79623466320928\n",
      "epoch: 22  train_loss: -50.81260035213718\n",
      "epoch: 23  train_loss: -50.80597339385147\n",
      "epoch: 24  train_loss: -50.785284699218074\n",
      "epoch: 25  train_loss: -50.808100874274636\n",
      "epoch: 26  train_loss: -50.83488316473472\n",
      "epoch: 27  train_loss: -50.829470188070864\n",
      "epoch: 28  train_loss: -50.83391383000136\n",
      "epoch: 29  train_loss: -50.83980162996411\n",
      "epoch: 30  train_loss: -50.83491102706016\n",
      "epoch: 31  train_loss: -50.83796754701794\n",
      "epoch: 32  train_loss: -50.8255450885206\n",
      "epoch: 33  train_loss: -50.85106197007928\n",
      "epoch: 34  train_loss: -50.83883849270261\n",
      "epoch: 35  train_loss: -50.8485043763139\n",
      "epoch: 36  train_loss: -50.85136161274154\n",
      "epoch: 37  train_loss: -50.84424513246711\n",
      "epoch: 38  train_loss: -50.83340083324193\n",
      "epoch: 39  train_loss: -50.85015532372419\n",
      "epoch: 40  train_loss: -50.854311965737814\n",
      "epoch: 41  train_loss: -50.842809309867945\n",
      "epoch: 42  train_loss: -50.84918427660161\n",
      "epoch: 43  train_loss: -50.85169367639969\n",
      "epoch: 44  train_loss: -50.841751638525935\n",
      "epoch: 45  train_loss: -50.85402172159833\n",
      "epoch: 46  train_loss: -50.85857808083845\n",
      "epoch: 47  train_loss: -50.860531684029006\n",
      "epoch: 48  train_loss: -50.857079728244294\n",
      "epoch: 49  train_loss: -50.85165811220123\n",
      "epoch: 50  train_loss: -50.86375862471973\n",
      "epoch: 51  train_loss: -50.863816945951214\n",
      "epoch: 52  train_loss: -50.854265380526286\n",
      "epoch: 53  train_loss: -50.86611690216887\n",
      "epoch: 54  train_loss: -50.84200423272913\n",
      "epoch: 55  train_loss: -50.85944969319345\n",
      "epoch: 56  train_loss: -50.854251604727\n",
      "epoch: 57  train_loss: -50.85285965319572\n",
      "epoch: 58  train_loss: -50.86501796528881\n",
      "epoch: 59  train_loss: -50.85230356747784\n",
      "epoch: 60  train_loss: -50.854740865180375\n",
      "epoch: 61  train_loss: -50.863783653850135\n",
      "epoch: 62  train_loss: -50.86525514700687\n",
      "epoch: 63  train_loss: -50.873334640337106\n",
      "epoch: 64  train_loss: -50.86029412185284\n",
      "epoch: 65  train_loss: -50.8634805151855\n",
      "epoch: 66  train_loss: -50.8683035883282\n",
      "epoch: 67  train_loss: -50.86009490107194\n",
      "epoch: 68  train_loss: -50.86975820812182\n",
      "epoch: 69  train_loss: -50.86428823225596\n",
      "epoch: 70  train_loss: -50.86694999909629\n",
      "epoch: 71  train_loss: -50.86718584831848\n",
      "epoch: 72  train_loss: -50.86355737529766\n",
      "epoch: 73  train_loss: -50.86933376724518\n",
      "epoch: 74  train_loss: -50.865217068905665\n",
      "epoch: 75  train_loss: -50.85686551651461\n",
      "epoch: 76  train_loss: -50.87061699539392\n",
      "epoch: 77  train_loss: -50.86283426351864\n",
      "epoch: 78  train_loss: -50.86236123863866\n",
      "epoch: 79  train_loss: -50.874490633712\n",
      "epoch: 80  train_loss: -50.858018772935665\n",
      "epoch: 81  train_loss: -50.867839896045005\n",
      "epoch: 82  train_loss: -50.8736100698933\n",
      "epoch: 83  train_loss: -50.87288449744453\n",
      "epoch: 84  train_loss: -50.87074839620654\n",
      "epoch: 85  train_loss: -50.85701060582234\n",
      "epoch: 86  train_loss: -50.87643774424187\n",
      "epoch: 87  train_loss: -50.86104500923765\n",
      "epoch: 88  train_loss: -50.86986766714539\n",
      "epoch: 89  train_loss: -50.872790816693524\n",
      "epoch: 90  train_loss: -50.871099691637085\n",
      "epoch: 91  train_loss: -50.874203866368156\n",
      "epoch: 92  train_loss: -50.87592290391511\n",
      "epoch: 93  train_loss: -50.8703503148469\n",
      "epoch: 94  train_loss: -50.86967191047711\n",
      "epoch: 95  train_loss: -50.86276652513976\n",
      "epoch: 96  train_loss: -50.87649923428969\n",
      "epoch: 97  train_loss: -50.873707104564176\n",
      "epoch: 98  train_loss: -50.8635926389485\n",
      "epoch: 99  train_loss: -50.87090125846275\n",
      "epoch: 100  train_loss: -50.87672281232331\n",
      "epoch: 101  train_loss: -50.87703610987647\n",
      "epoch: 102  train_loss: -50.87345372358842\n",
      "epoch: 103  train_loss: -50.87152308325936\n",
      "epoch: 104  train_loss: -50.875267077930516\n",
      "epoch: 105  train_loss: -50.87602756276756\n",
      "epoch: 106  train_loss: -50.868912293939445\n",
      "epoch: 107  train_loss: -50.87788852004168\n",
      "epoch: 108  train_loss: -50.870997181803624\n",
      "epoch: 109  train_loss: -50.87563723214861\n",
      "epoch: 110  train_loss: -50.87656503895191\n",
      "epoch: 111  train_loss: -50.877326870555635\n",
      "epoch: 112  train_loss: -50.86721997575616\n",
      "epoch: 113  train_loss: -50.8809936077043\n",
      "epoch: 114  train_loss: -50.88049809106216\n",
      "epoch: 115  train_loss: -50.87459688945197\n",
      "epoch: 116  train_loss: -50.880136781144664\n",
      "epoch: 117  train_loss: -50.8711202167977\n",
      "epoch: 118  train_loss: -50.874976550415596\n",
      "epoch: 119  train_loss: -50.8827823083936\n",
      "epoch: 120  train_loss: -50.867638780956\n",
      "epoch: 121  train_loss: -50.887398443129406\n",
      "epoch: 122  train_loss: -50.88166948291303\n",
      "epoch: 123  train_loss: -50.8773118272127\n",
      "epoch: 124  train_loss: -50.87685744599854\n",
      "epoch: 125  train_loss: -50.88545409177675\n",
      "epoch: 126  train_loss: -50.878484688785235\n",
      "epoch: 127  train_loss: -50.87833455398195\n",
      "epoch: 128  train_loss: -50.881019424386736\n",
      "epoch: 129  train_loss: -50.879346365724196\n",
      "epoch: 130  train_loss: -50.88683311396269\n",
      "epoch: 131  train_loss: -50.879700054523404\n",
      "epoch: 132  train_loss: -50.877627848274344\n",
      "epoch: 133  train_loss: -50.87319700938258\n",
      "epoch: 134  train_loss: -50.88494699633099\n",
      "epoch: 135  train_loss: -50.87583211057596\n",
      "epoch: 136  train_loss: -50.87987289175015\n",
      "epoch: 137  train_loss: -50.876411312643135\n",
      "epoch: 138  train_loss: -50.8849700590057\n",
      "epoch: 139  train_loss: -50.880104338782566\n",
      "epoch: 140  train_loss: -50.882518996875056\n",
      "epoch: 141  train_loss: -50.87849901839686\n",
      "epoch: 142  train_loss: -50.88339425858503\n",
      "epoch: 143  train_loss: -50.87471115702317\n",
      "epoch: 144  train_loss: -50.878767836198456\n",
      "epoch: 145  train_loss: -50.875825186139785\n",
      "epoch: 146  train_loss: -50.88282381862687\n",
      "epoch: 147  train_loss: -50.880706107109674\n",
      "epoch: 148  train_loss: -50.87647826489772\n",
      "epoch: 149  train_loss: -50.881634318167215\n",
      "epoch: 150  train_loss: -50.882439191269455\n",
      "epoch: 151  train_loss: -50.882233939580225\n",
      "epoch: 152  train_loss: -50.880728709216626\n",
      "epoch: 153  train_loss: -50.879997395617785\n",
      "epoch: 154  train_loss: -50.86898430840593\n",
      "epoch: 155  train_loss: -50.887248967246\n",
      "epoch: 156  train_loss: -50.878776596101126\n",
      "epoch: 157  train_loss: -50.88140377472431\n",
      "epoch: 158  train_loss: -50.871880603528545\n",
      "epoch: 159  train_loss: -50.876398391477494\n",
      "epoch: 160  train_loss: -50.88994204871576\n",
      "epoch: 161  train_loss: -50.883508461150406\n",
      "epoch: 162  train_loss: -50.88153872347256\n",
      "epoch: 163  train_loss: -50.876923171940504\n",
      "epoch: 164  train_loss: -50.8839040860643\n",
      "epoch: 165  train_loss: -50.88260856649693\n",
      "epoch: 166  train_loss: -50.88060013643717\n",
      "epoch: 167  train_loss: -50.88751349648237\n",
      "epoch: 168  train_loss: -50.87999770288058\n",
      "epoch: 169  train_loss: -50.886060902664994\n",
      "epoch: 170  train_loss: -50.88588788680861\n",
      "epoch: 171  train_loss: -50.886110658571766\n",
      "epoch: 172  train_loss: -50.88023636151119\n",
      "epoch: 173  train_loss: -50.87908501005549\n",
      "epoch: 174  train_loss: -50.8724263673496\n",
      "epoch: 175  train_loss: -50.88954549379523\n",
      "epoch: 176  train_loss: -50.88489965459159\n",
      "epoch: 177  train_loss: -50.890131964614945\n",
      "epoch: 178  train_loss: -50.887224954242484\n",
      "epoch: 179  train_loss: -50.8903645445428\n",
      "epoch: 180  train_loss: -50.87837487988054\n",
      "epoch: 181  train_loss: -50.88950068107958\n",
      "epoch: 182  train_loss: -50.881367152193015\n",
      "epoch: 183  train_loss: -50.88410545657083\n",
      "epoch: 184  train_loss: -50.87727864426923\n",
      "epoch: 185  train_loss: -50.883836258002034\n",
      "epoch: 186  train_loss: -50.88523246728058\n",
      "epoch: 187  train_loss: -50.884228567638175\n",
      "epoch: 188  train_loss: -50.88910219113768\n",
      "epoch: 189  train_loss: -50.885714094498184\n",
      "epoch: 190  train_loss: -50.88952239613302\n",
      "epoch: 191  train_loss: -50.8780909142982\n",
      "epoch: 192  train_loss: -50.88333656285023\n",
      "epoch: 193  train_loss: -50.891584286839624\n",
      "epoch: 194  train_loss: -50.885310427074174\n",
      "epoch: 195  train_loss: -50.8877970202297\n",
      "epoch: 196  train_loss: -50.883900715438116\n",
      "epoch: 197  train_loss: -50.88737301022808\n",
      "epoch: 198  train_loss: -50.88762622783376\n",
      "epoch: 199  train_loss: -50.88553862815613\n",
      "epoch: 200  train_loss: -50.8925090248601\n",
      "epoch: 201  train_loss: -50.886596932538666\n",
      "epoch: 202  train_loss: -50.88699836871358\n",
      "epoch: 203  train_loss: -50.88825996986948\n",
      "epoch: 204  train_loss: -50.88913286881804\n",
      "epoch: 205  train_loss: -50.88325020743251\n",
      "epoch: 206  train_loss: -50.88494125877709\n",
      "epoch: 207  train_loss: -50.88547346526631\n",
      "epoch: 208  train_loss: -50.887712830004034\n",
      "epoch: 209  train_loss: -50.885249375655896\n",
      "epoch: 210  train_loss: -50.88637949614508\n",
      "epoch: 211  train_loss: -50.882795117032884\n",
      "epoch: 212  train_loss: -50.891083292618895\n",
      "epoch: 213  train_loss: -50.892486373557084\n",
      "epoch: 214  train_loss: -50.88769117744279\n",
      "epoch: 215  train_loss: -50.891979488348376\n",
      "epoch: 216  train_loss: -50.889408672296\n",
      "epoch: 217  train_loss: -50.89114314224558\n",
      "epoch: 218  train_loss: -50.887183467517346\n",
      "epoch: 219  train_loss: -50.886949009059315\n",
      "epoch: 220  train_loss: -50.88450711013117\n",
      "epoch: 221  train_loss: -50.88553649042838\n",
      "epoch: 222  train_loss: -50.892549988310876\n",
      "epoch: 223  train_loss: -50.89576553284748\n",
      "epoch: 224  train_loss: -50.8908635479112\n",
      "epoch: 225  train_loss: -50.888060452352555\n",
      "epoch: 226  train_loss: -50.892727033265956\n",
      "epoch: 227  train_loss: -50.88430494278105\n",
      "epoch: 228  train_loss: -50.89015474496206\n",
      "epoch: 229  train_loss: -50.888601680053554\n",
      "epoch: 230  train_loss: -50.8878027727927\n",
      "epoch: 231  train_loss: -50.886294606254886\n",
      "epoch: 232  train_loss: -50.88744716329878\n",
      "epoch: 233  train_loss: -50.890040321349815\n",
      "epoch: 234  train_loss: -50.8916398822196\n",
      "epoch: 235  train_loss: -50.88373459938515\n",
      "epoch: 236  train_loss: -50.89137860568826\n",
      "epoch: 237  train_loss: -50.8926120485664\n",
      "epoch: 238  train_loss: -50.891568011734115\n",
      "epoch: 239  train_loss: -50.89428856459305\n",
      "epoch: 240  train_loss: -50.87777055446791\n",
      "epoch: 241  train_loss: -50.88793697184756\n",
      "epoch: 242  train_loss: -50.88718727808403\n",
      "epoch: 243  train_loss: -50.88952920787898\n",
      "epoch: 244  train_loss: -50.88956963734968\n",
      "epoch: 245  train_loss: -50.8963338671739\n",
      "epoch: 246  train_loss: -50.8923757654879\n",
      "epoch: 247  train_loss: -50.887224215980076\n",
      "epoch: 248  train_loss: -50.88976383176369\n",
      "epoch: 249  train_loss: -50.88720540796133\n",
      "epoch: 250  train_loss: -50.89451150245017\n",
      "epoch: 251  train_loss: -50.8959237916858\n",
      "epoch: 252  train_loss: -50.88987177370215\n",
      "epoch: 253  train_loss: -50.89092686873027\n",
      "epoch: 254  train_loss: -50.88978131921955\n",
      "epoch: 255  train_loss: -50.89167714284173\n",
      "epoch: 256  train_loss: -50.89151612475651\n",
      "epoch: 257  train_loss: -50.895069701685955\n",
      "epoch: 258  train_loss: -50.89174435533611\n",
      "epoch: 259  train_loss: -50.8900564931745\n",
      "epoch: 260  train_loss: -50.886183430793515\n",
      "epoch: 261  train_loss: -50.896960942616424\n",
      "epoch: 262  train_loss: -50.88977445570174\n",
      "epoch: 263  train_loss: -50.88753682856215\n",
      "epoch: 264  train_loss: -50.89561067265889\n",
      "epoch: 265  train_loss: -50.896311653298966\n",
      "epoch: 266  train_loss: -50.891215310339916\n",
      "epoch: 267  train_loss: -50.89069207556608\n",
      "epoch: 268  train_loss: -50.88843982524415\n",
      "epoch: 269  train_loss: -50.8879283686648\n",
      "epoch: 270  train_loss: -50.8957599876315\n",
      "epoch: 271  train_loss: -50.89091534294466\n",
      "epoch: 272  train_loss: -50.89448787998501\n",
      "epoch: 273  train_loss: -50.89120414563165\n",
      "epoch: 274  train_loss: -50.894317598011504\n",
      "epoch: 275  train_loss: -50.88660210660191\n",
      "epoch: 276  train_loss: -50.8984925264469\n",
      "epoch: 277  train_loss: -50.89224711193668\n",
      "epoch: 278  train_loss: -50.89325574901048\n",
      "epoch: 279  train_loss: -50.88815529223994\n",
      "epoch: 280  train_loss: -50.89434719238183\n",
      "epoch: 281  train_loss: -50.892692550919826\n",
      "epoch: 282  train_loss: -50.889133902889306\n",
      "epoch: 283  train_loss: -50.89408971073037\n",
      "epoch: 284  train_loss: -50.889622501423574\n",
      "epoch: 285  train_loss: -50.89121106012243\n",
      "epoch: 286  train_loss: -50.894967032389424\n",
      "epoch: 287  train_loss: -50.89368551724166\n",
      "epoch: 288  train_loss: -50.89263562507092\n",
      "epoch: 289  train_loss: -50.89069341116802\n",
      "epoch: 290  train_loss: -50.89226171678005\n",
      "epoch: 291  train_loss: -50.89750438841924\n",
      "epoch: 292  train_loss: -50.896063979149446\n",
      "epoch: 293  train_loss: -50.8975120399583\n",
      "epoch: 294  train_loss: -50.89366561491099\n",
      "epoch: 295  train_loss: -50.894718415330004\n",
      "epoch: 296  train_loss: -50.89559532381311\n",
      "epoch: 297  train_loss: -50.894918802696814\n",
      "epoch: 298  train_loss: -50.890054918252034\n",
      "epoch: 299  train_loss: -50.894072196042636\n",
      "epoch: 300  train_loss: -50.88905439173319\n",
      "epoch: 301  train_loss: -50.893454398234994\n",
      "epoch: 302  train_loss: -50.89766503057166\n",
      "epoch: 303  train_loss: -50.895162780415404\n",
      "epoch: 304  train_loss: -50.89180436207728\n",
      "epoch: 305  train_loss: -50.889825640521565\n",
      "epoch: 306  train_loss: -50.89756851393968\n",
      "epoch: 307  train_loss: -50.89179483341714\n",
      "epoch: 308  train_loss: -50.89401235828261\n",
      "epoch: 309  train_loss: -50.89270829233861\n",
      "epoch: 310  train_loss: -50.89079848820999\n",
      "epoch: 311  train_loss: -50.892763621094545\n",
      "epoch: 312  train_loss: -50.893981245172824\n",
      "epoch: 313  train_loss: -50.88497942577991\n",
      "epoch: 314  train_loss: -50.89620565153008\n",
      "epoch: 315  train_loss: -50.893184599401806\n",
      "epoch: 316  train_loss: -50.89633028686461\n",
      "epoch: 317  train_loss: -50.89832940970838\n",
      "epoch: 318  train_loss: -50.89715451851672\n",
      "epoch: 319  train_loss: -50.89787998459794\n",
      "epoch: 320  train_loss: -50.89674788141018\n",
      "epoch: 321  train_loss: -50.888799525202096\n",
      "epoch: 322  train_loss: -50.89359144630205\n",
      "epoch: 323  train_loss: -50.89933235437862\n",
      "epoch: 324  train_loss: -50.895557208486345\n",
      "epoch: 325  train_loss: -50.89591913872863\n",
      "epoch: 326  train_loss: -50.89805194280793\n",
      "epoch: 327  train_loss: -50.89912800589865\n",
      "epoch: 328  train_loss: -50.892695776225786\n",
      "epoch: 329  train_loss: -50.89307592815521\n",
      "epoch: 330  train_loss: -50.89114649738836\n",
      "epoch: 331  train_loss: -50.897632419270394\n",
      "epoch: 332  train_loss: -50.88842389060242\n",
      "epoch: 333  train_loss: -50.89406930099092\n",
      "epoch: 334  train_loss: -50.89766169227557\n",
      "epoch: 335  train_loss: -50.900867519409054\n",
      "epoch: 336  train_loss: -50.89114734926723\n",
      "epoch: 337  train_loss: -50.89652663787182\n",
      "epoch: 338  train_loss: -50.899430237697146\n",
      "epoch: 339  train_loss: -50.890888757677445\n",
      "epoch: 340  train_loss: -50.89786020019981\n",
      "epoch: 341  train_loss: -50.89619545958053\n",
      "epoch: 342  train_loss: -50.89570332353982\n",
      "epoch: 343  train_loss: -50.89722135912965\n",
      "epoch: 344  train_loss: -50.8902806154083\n",
      "epoch: 345  train_loss: -50.89491084067956\n",
      "epoch: 346  train_loss: -50.896883741841215\n",
      "epoch: 347  train_loss: -50.89467931607272\n",
      "epoch: 348  train_loss: -50.89324550584291\n",
      "epoch: 349  train_loss: -50.89944871931908\n",
      "epoch: 350  train_loss: -50.8985866233097\n",
      "epoch: 351  train_loss: -50.89552813992505\n",
      "epoch: 352  train_loss: -50.89756937166853\n",
      "epoch: 353  train_loss: -50.89726022618727\n",
      "epoch: 354  train_loss: -50.900007326898006\n",
      "epoch: 355  train_loss: -50.89877171970348\n",
      "epoch: 356  train_loss: -50.89197677792608\n",
      "epoch: 357  train_loss: -50.89898935523172\n",
      "epoch: 358  train_loss: -50.89345670211721\n",
      "epoch: 359  train_loss: -50.89788793777652\n",
      "epoch: 360  train_loss: -50.897149303144445\n",
      "epoch: 361  train_loss: -50.90174640181324\n",
      "epoch: 362  train_loss: -50.89679323562468\n",
      "epoch: 363  train_loss: -50.89159314102817\n",
      "epoch: 364  train_loss: -50.89496604600775\n",
      "epoch: 365  train_loss: -50.8961910522569\n",
      "Early stopping!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTYPE = torch.float64\n",
    "DEVICE = torch.device=\"cpu\"\n",
    "batch_size=200\n",
    "\n",
    "X = data.X\n",
    "eq_cm = data.eq_cm\n",
    "ineq_cm = data.ineq_cm\n",
    "eq_rhs = data.eq_rhs\n",
    "ineq_rhs = data.ineq_rhs\n",
    "\n",
    "Q = data.Q\n",
    "p = data.p\n",
    "\n",
    "print(f\"eq_cm.shape: {eq_cm.shape}\")\n",
    "print(f\"ineq_cm.shape: {ineq_cm.shape}\")\n",
    "print(f\"eq_rhs.shape: {eq_rhs.shape}\")\n",
    "print(f\"ineq_rhs.shape: {ineq_rhs.shape}\")\n",
    "print(f\"Q.shape: {Q.shape}\")\n",
    "print(f\"p.shape: {p.shape}\")\n",
    "\n",
    "train = data.train_indices\n",
    "valid = data.valid_indices\n",
    "test = data.test_indices\n",
    "\n",
    "# Traning data in a data set\n",
    "#! Vary per experiment\n",
    "\n",
    "print(eq_rhs[data.train_indices].shape)\n",
    "\n",
    "samples_train = {\"eq_rhs\": eq_rhs[data.train_indices]}\n",
    "samples_valid = {\"eq_rhs\": eq_rhs[data.valid_indices]}\n",
    "samples_test = {\"eq_rhs\": eq_rhs[data.test_indices]}\n",
    "\n",
    "train_data = nm.dataset.DictDataset(samples_train, name='train')\n",
    "valid_data = nm.dataset.DictDataset(samples_valid, name='dev')\n",
    "test_data = nm.dataset.DictDataset(samples_test, name='test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           collate_fn=train_data.collate_fn, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
    "                                         collate_fn=valid_data.collate_fn, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                         collate_fn=test_data.collate_fn, shuffle=True)\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "func = nm.blocks.MLP(insize=data.xdim, outsize=data.ydim, bias=True, linear_map=nm.slim.maps['linear'], nonlin=nn.ReLU, hsizes=[500, 500])\n",
    "\n",
    "sol_map = nm.Node(func, input_keys=['eq_rhs'], output_keys=['y'], name='map')\n",
    "\n",
    "y = nm.variable(\"y\")\n",
    "\n",
    "eq_rhs = nm.variable(\"eq_rhs\")\n",
    "\n",
    "#1/2 * y^T Q y + p^Ty\n",
    "# f = 0.5 * y @ Q @ y.T + p @ y\n",
    "f = torch.sum(0.5 * (y @ Q) * y +  p * y, dim=1)\n",
    "obj = f.minimize(weight=1.0, name=\"obj\")\n",
    "\n",
    "# eq_constraints = eq_cm @ y - eq_rhs == 0\n",
    "eq_constraints = y @ eq_cm.T - eq_rhs == 0\n",
    "ineq_constraints = y @ ineq_cm.T - ineq_rhs.view(1, -1) <= 0  # Broadcast ineq_rhs\n",
    "# ineq_constraints = ineq_cm @ y - ineq_rhs <= 0\n",
    "\n",
    "eq_constraints.name = 'eq_constraints'\n",
    "ineq_constraints.name = 'ineq_constraints'\n",
    "\n",
    "objectives = [obj]\n",
    "constraints = [eq_constraints, ineq_constraints]\n",
    "components = [sol_map]\n",
    "\n",
    "loss = nm.PenaltyLoss(objectives, constraints)\n",
    "\n",
    "problem = nm.Problem(components, loss)\n",
    "\n",
    "lr = 0.001      # step size for gradient descent\n",
    "epochs = 400    # number of training epochs\n",
    "warmup = 100    # number of epochs to wait before enacting early stopping policy\n",
    "patience = 100  # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=lr)\n",
    "\n",
    "# define trainer\n",
    "trainer = nm.Trainer(\n",
    "    problem,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    warmup=warmup)\n",
    "\n",
    "# Train NLP solution map\n",
    "best_model = trainer.train()\n",
    "best_outputs = trainer.test(best_model)\n",
    "# load best model dict\n",
    "problem.load_state_dict(best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjacobs/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:169: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([200, 50])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(left, right)\n"
     ]
    }
   ],
   "source": [
    "for test_data in test_loader:\n",
    "    model_out = problem(test_data)\n",
    "\n",
    "data.obj_fn(model_out['test_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 42\u001b[0m\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m nm\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     32\u001b[0m     problem,\n\u001b[1;32m     33\u001b[0m     train_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     patience\u001b[38;5;241m=\u001b[39mpatience,\n\u001b[1;32m     39\u001b[0m     warmup\u001b[38;5;241m=\u001b[39mwarmup)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Train NLP solution map\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m best_outputs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(best_model)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# load best model dict\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/trainer.py:246\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m t_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    245\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m move_batch_to_device(t_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 246\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_fidelity:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnodes:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/problem.py:199\u001b[0m, in \u001b[0;36mProblem.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    198\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(data)\n\u001b[0;32m--> 199\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    201\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mname: output_dict}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/loss.py:177\u001b[0m, in \u001b[0;36mPenaltyLoss.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    175\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobjectives_dict}\n\u001b[1;32m    176\u001b[0m fx \u001b[38;5;241m=\u001b[39m objectives_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 177\u001b[0m penalties_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_constraints\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpenalties_dict}\n\u001b[1;32m    179\u001b[0m penalties \u001b[38;5;241m=\u001b[39m penalties_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpenalty_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/loss.py:87\u001b[0m, in \u001b[0;36mAggregateLoss.calculate_constraints\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m eq_flags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# get loss, values, and violations of constraint via its forward pass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput}\n\u001b[1;32m     89\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output[c\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:314\u001b[0m, in \u001b[0;36mConstraint.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m:param input_dict: (dict, {str: torch.Tensor}) Should contain keys corresponding to self.variable_names\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m:return: 0-dimensional torch.Tensor that can be cast as a floating point number\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, Variable):\n\u001b[0;32m--> 314\u001b[0m     left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(left, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    316\u001b[0m         left \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(left)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:551\u001b[0m, in \u001b[0;36mVariable.forward\u001b[0;34m(self, datadict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_nodes:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_value(n, datadict)\n\u001b[0;32m--> 551\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatadict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:558\u001b[0m, in \u001b[0;36mVariable.get_value\u001b[0;34m(self, n, datadict)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         args \u001b[38;5;241m=\u001b[39m [src\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;28;01mfor\u001b[39;00m src, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_g\u001b[38;5;241m.\u001b[39min_edges(n)]\n\u001b[0;32m--> 558\u001b[0m         n\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     n\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m datadict[n\u001b[38;5;241m.\u001b[39m_key]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:454\u001b[0m, in \u001b[0;36mVariable.__sub__.<locals>.<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m, other], func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m, display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train NLP solution map\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m best_outputs \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(best_model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# load best model dict\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/trainer.py:246\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m t_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m    245\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m move_batch_to_device(t_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 246\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_fidelity:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnodes:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/problem.py:199\u001b[0m, in \u001b[0;36mProblem.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    198\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(data)\n\u001b[0;32m--> 199\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_dict, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    201\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mname: output_dict}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/loss.py:177\u001b[0m, in \u001b[0;36mPenaltyLoss.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    175\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobjectives_dict}\n\u001b[1;32m    176\u001b[0m fx \u001b[38;5;241m=\u001b[39m objectives_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 177\u001b[0m penalties_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_constraints\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpenalties_dict}\n\u001b[1;32m    179\u001b[0m penalties \u001b[38;5;241m=\u001b[39m penalties_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpenalty_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/loss.py:87\u001b[0m, in \u001b[0;36mAggregateLoss.calculate_constraints\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m eq_flags \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# get loss, values, and violations of constraint via its forward pass\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     output_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutput}\n\u001b[1;32m     89\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output[c\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:314\u001b[0m, in \u001b[0;36mConstraint.forward\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m:param input_dict: (dict, {str: torch.Tensor}) Should contain keys corresponding to self.variable_names\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m:return: 0-dimensional torch.Tensor that can be cast as a floating point number\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, Variable):\n\u001b[0;32m--> 314\u001b[0m     left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(left, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    316\u001b[0m         left \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(left)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:551\u001b[0m, in \u001b[0;36mVariable.forward\u001b[0;34m(self, datadict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_nodes:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_value(n, datadict)\n\u001b[0;32m--> 551\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatadict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:558\u001b[0m, in \u001b[0;36mVariable.get_value\u001b[0;34m(self, n, datadict)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39m_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         args \u001b[38;5;241m=\u001b[39m [src\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;28;01mfor\u001b[39;00m src, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_g\u001b[38;5;241m.\u001b[39min_edges(n)]\n\u001b[0;32m--> 558\u001b[0m         n\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m     n\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m datadict[n\u001b[38;5;241m.\u001b[39m_key]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/neuromancer/constraint.py:454\u001b[0m, in \u001b[0;36mVariable.__sub__.<locals>.<lambda>\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable(input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m, other], func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m, display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-neuromancer",
   "language": "python",
   "name": "thesis-neuromancer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

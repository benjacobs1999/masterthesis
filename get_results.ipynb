{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimalNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        \n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes + [data.ydim]\n",
    "        layers = []\n",
    "\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            if out_size != data.ydim:  # Add ReLU activation for hidden layers only\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DualNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes, mu_size, lamb_size):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        self._mu_size = mu_size\n",
    "        self._lamb_size = lamb_size\n",
    "\n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes\n",
    "        # layer_sizes = [2*data.xdim + 1000] + self._hidden_sizes\n",
    "        layers = []\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        # Add the output layer\n",
    "        self.out_layer = nn.Linear(self._hidden_sizes[-1], self._mu_size + self._lamb_size)\n",
    "        nn.init.zeros_(self.out_layer.weight)  # Initialize output layer weights to 0\n",
    "        nn.init.zeros_(self.out_layer.bias)    # Initialize output layer biases to 0\n",
    "        layers.append(self.out_layer)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out_mu = out[:, :self._mu_size]\n",
    "        out_lamb = out[:, self._mu_size:]\n",
    "        return out_mu, out_lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"QP_data/Dual_QP_simple_dataset_var100_ineq50_eq50_ex10000\", 'rb') as f:\n",
    "    original_data = pickle.load(f)\n",
    "\n",
    "with open(\"QP_data/Varying_G_type=row_dataset_var100_ineq50_eq50_ex10000\", 'rb') as f:\n",
    "    varying_cm_row_data = pickle.load(f)\n",
    "\n",
    "with open(\"QP_data/Varying_G_type=column_dataset_var100_ineq50_eq50_ex10000\", 'rb') as f:\n",
    "    varying_cm_column_data = pickle.load(f)\n",
    "\n",
    "with open(\"QP_data/Varying_G_type=random_dataset_var100_ineq50_eq50_ex10000\", 'rb') as f:\n",
    "    varying_cm_random_data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_primal_net = PrimalNet(original_data, [500, 500])\n",
    "row_primal_net = PrimalNet(varying_cm_row_data, [500, 500])\n",
    "col_primal_net = PrimalNet(varying_cm_column_data, [500, 500])\n",
    "random_primal_net = PrimalNet(varying_cm_random_data, [500, 500])\n",
    "# Load state dictionaries.\n",
    "original_primal_state_dict = torch.load(\"benchmark_experiment_output/original/0.006_primal_net.dict\", weights_only=True)\n",
    "row_primal_state_dict = torch.load(\"benchmark_experiment_output/row/0.15_primal_net.dict\", weights_only=True)\n",
    "col_primal_state_dict = torch.load(\"benchmark_experiment_output/column/0.08_primal_net.dict\", weights_only=True)\n",
    "random_primal_state_dict = torch.load(\"benchmark_experiment_output/random/0.08_primal_net.dict\", weights_only=True)\n",
    "\n",
    "# Load the state dictionaries into the networks.\n",
    "original_primal_net.load_state_dict(original_primal_state_dict)\n",
    "row_primal_net.load_state_dict(row_primal_state_dict)\n",
    "col_primal_net.load_state_dict(col_primal_state_dict)\n",
    "random_primal_net.load_state_dict(random_primal_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.037454694128508 -14.986555323269725 0.003388805568463467 0.0025407374717075935 0.00022056134756535966 0.005797883440416547 0.001874068337470995\n",
      "-15.57003679726652 -15.565470437540043 0.00028990328345969435 0.14287585365932684 0.003998443021954674 0.059967531574818066 0.015439284315856441\n",
      "-15.357705954282094 -13.367091352379102 0.1296201915871199 0.07148420878412369 0.002151673491094704 0.024722809905760316 0.0077619844199713055\n",
      "-15.602404350014051 -14.55370142820193 0.06708270892571695 0.08031144552102452 0.002277259257184765 0.006866528529356717 0.00221524296613162\n"
     ]
    }
   ],
   "source": [
    "for model, data in [(original_primal_net, original_data), (row_primal_net, varying_cm_row_data), (col_primal_net, varying_cm_column_data), (random_primal_net, varying_cm_random_data)]:\n",
    "    X = data.X[data.test_indices]\n",
    "    Y_pred = model(X)\n",
    "    obj_known = data.obj_fn(data.Y[data.test_indices]).detach().cpu().numpy()\n",
    "    obj_pred = data.obj_fn(Y_pred).detach().cpu().numpy()\n",
    "    obj_gap = ((obj_known - obj_pred)/obj_known).mean()\n",
    "\n",
    "    ineq_dist = data.ineq_dist(X, Y_pred)\n",
    "    eq_resid = data.eq_resid(X, Y_pred)\n",
    "\n",
    "    ineq_max_vals = torch.max(ineq_dist, dim=1)[0].detach().cpu().numpy().mean()\n",
    "    ineq_mean_vals = torch.mean(ineq_dist, dim=1).detach().cpu().numpy().mean()\n",
    "    eq_max_vals = torch.max(torch.abs(eq_resid), dim=1)[0].detach().cpu().numpy().mean()\n",
    "    eq_mean_vals = torch.mean(torch.abs(eq_resid), dim=1).detach().cpu().numpy().mean()\n",
    "    \n",
    "    print(obj_known.mean(), obj_pred.mean(), obj_gap, ineq_max_vals, ineq_mean_vals, eq_max_vals, eq_mean_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

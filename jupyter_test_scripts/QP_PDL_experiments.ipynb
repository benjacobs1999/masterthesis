{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! Taken from https://github.com/locuslab/DC3/blob/main/datasets/simple/make_dataset.py\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import osqp\n",
    "from qpth.qp import QPFunction\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from QP_problem import SimpleProblem\n",
    "from primal_dual import PrimalDualTrainer\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalQPProblem(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G, b, d, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G, b, d, valid_frac, test_frac)\n",
    "\n",
    "        self._X = self._b\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        self._nineq = self._G.shape[0]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        return X - Y @ self.A.T\n",
    "\n",
    "    def ineq_resid(self, X, Y):\n",
    "        return Y @ self.G.T - self.d\n",
    "\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, A, G, d = self.Q_np, self.p_np, self.A_np, self.G_np, self.d_np\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            for Xi in X_np:\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, G])\n",
    "                my_l = np.hstack([Xi, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([Xi, d])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "                sols = np.array(Y)\n",
    "                parallel_time = total_time / len(X_np)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "\n",
    "class QPProblemVaryingG(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G_base, G_varying, b, d, n_varying_rows, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G_varying, b, d, valid_frac, test_frac)\n",
    "        self.G_base = torch.tensor(G_base)\n",
    "        self.n_varying_rows = n_varying_rows\n",
    "        # Take the first n rows of G as input\n",
    "        self._X = self._G[:, :n_varying_rows, :].flatten(start_dim=1)\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        # G now has num_samples in first dimension, num_constraints in second dimension. Take second dimension!\n",
    "        self._nineq = self._G.shape[1]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        \"\"\"RHS of equality constraints now remains constant across problem instances.\"\"\"\n",
    "        return self.b - Y @ self.A.T\n",
    "\n",
    "    def rebuild_G_from_X(self, X):\n",
    "        # Reshape X to match the first self.n_varying_rows rows of G\n",
    "        custom_G = X.reshape(X.shape[0], self.n_varying_rows, self._ydim)  # Reshape for the batch size\n",
    "\n",
    "        # Take only the first sample of G and clone it for modification\n",
    "        G = self.G_base.clone()  # Shape is (M, P)\n",
    "\n",
    "        # Repeat G for the batch size to avoid memory overlap\n",
    "        G = G.unsqueeze(0).repeat(X.shape[0], 1, 1)  # Shape is (batch_size, M, P)\n",
    "\n",
    "        # Assign custom_G to the first self.n_varying_rows rows of G\n",
    "        G[:, :self.n_varying_rows, :] = custom_G  # Ensure dimensions match\n",
    "        return G\n",
    "    \n",
    "    def ineq_resid(self, X, Y):\n",
    "        \"\"\"\n",
    "        For the ineq resid, we need to extract the first n rows of the G matrix from it's flattened form X, and plug them into G.\n",
    "        \"\"\"\n",
    "\n",
    "        G = self.rebuild_G_from_X(X)\n",
    "\n",
    "        # resid = Y @ G.transpose(1, 2) - h\n",
    "        residual = torch.bmm(Y.unsqueeze(1), G.transpose(1, 2)).squeeze(1) - self.d\n",
    "\n",
    "        # Compute inequality residual\n",
    "        return residual\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        \"\"\"We change op_solve so that the varying G matrices are taken from the input X.\n",
    "        \"\"\"\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, b, d = self.Q_np, self.p_np, self.b_np, self.d_np\n",
    "            G_np = self.rebuild_G_from_X(X).detach().cpu().numpy()\n",
    "            A = self.A_np\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            for Gi in G_np:\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, Gi])\n",
    "                my_l = np.hstack([b, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([b, d])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "            sols = np.array(Y)\n",
    "            parallel_time = total_time / len(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "\n",
    "class QPProblemVaryingGbd(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G_base, G_varying, b, d, n_varying_rows, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G_varying, b, d, valid_frac, test_frac)\n",
    "        self.G_base = torch.tensor(G_base)\n",
    "        self.n_varying_rows = n_varying_rows\n",
    "        # Flatten the rows of G that are varying, to be added to the NN input.\n",
    "        G_flattened = self.G[:, :n_varying_rows, :].flatten(start_dim=1)\n",
    "        self._X = torch.concat([G_flattened, self.b, self.d], dim=1)\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        # G now has num_samples in first dimension, num_constraints in second dimension. Take second dimension!\n",
    "        self._nineq = self._G.shape[1]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        \"\"\"B is now varying, we should extract it from X\"\"\"\n",
    "        G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "        return b - Y @ self.A.T\n",
    "\n",
    "    def rebuild_Gbd_from_X(self, X):\n",
    "        # Reshape X to match the first self.n_varying_rows rows of G\n",
    "        G_size = self.n_varying_rows*self.ydim\n",
    "        b_size = self.neq\n",
    "        flattened_G = X[:, :G_size]\n",
    "        b = X[:, G_size:G_size+b_size]\n",
    "        d = X[:, G_size+b_size:]\n",
    "        custom_G = flattened_G.reshape(X.shape[0], self.n_varying_rows, self._ydim)  # Reshape for the batch size\n",
    "\n",
    "        # Take only the first sample of G and clone it for modification\n",
    "        G = self.G_base.clone()  # Shape is (M, P)\n",
    "\n",
    "        # Repeat G for the batch size to avoid memory overlap\n",
    "        G = G.unsqueeze(0).repeat(X.shape[0], 1, 1)  # Shape is (batch_size, M, P)\n",
    "\n",
    "        # Assign custom_G to the first self.n_varying_rows rows of G\n",
    "        G[:, :self.n_varying_rows, :] = custom_G  # Ensure dimensions match\n",
    "        return G, b, d\n",
    "    \n",
    "    def ineq_resid(self, X, Y):\n",
    "        \"\"\"\n",
    "        For the ineq resid, we need to extract the first n rows of the G matrix from it's flattened form X, and plug them into G.\n",
    "        \"\"\"\n",
    "\n",
    "        G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "\n",
    "        # resid = Y @ G.transpose(1, 2) - h\n",
    "        residual = torch.bmm(Y.unsqueeze(1), G.transpose(1, 2)).squeeze(1) - d\n",
    "\n",
    "        # Compute inequality residual\n",
    "        return residual\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        \"\"\"We change op_solve so that the varying G matrices are taken from the input X.\n",
    "        \"\"\"\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, b, d = self.Q_np, self.p_np, self.b_np, self.d_np\n",
    "            G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "            G_np, b_np, d_np = G.detach().cpu().numpy(), b.detach().cpu().numpy(), d.detach().cpu().numpy()\n",
    "            A = self.A_np\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            for idx, Gi in enumerate(G_np):\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, Gi])\n",
    "                my_l = np.hstack([b_np[idx], -np.ones(d_np[idx].shape[0]) * np.inf])\n",
    "                my_u = np.hstack([b_np[idx], d_np[idx]])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "            sols = np.array(Y)\n",
    "            parallel_time = total_time / len(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "    \n",
    "\n",
    "class ScaledLPProblem(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G, b, d, obj_coeff, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G, b, d, valid_frac, test_frac)\n",
    "\n",
    "        self._X = self._b\n",
    "        self._c = torch.tensor(obj_coeff)\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        self._nineq = self._G.shape[0]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        return X - Y @ self.A.T\n",
    "\n",
    "    def ineq_resid(self, X, Y):\n",
    "        return Y @ self.G.T - self.d\n",
    "\n",
    "    def obj_fn(self, Y):\n",
    "        return Y @ self._c.T\n",
    "\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, A, G, d = self.Q_np, self.p_np, self.A_np, self.G_np, self.d_np\n",
    "            c = self._c.numpy()\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            zero_Q = np.zeros((c.shape[0], c.shape[0]))\n",
    "\n",
    "            for Xi in X_np:\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, G])\n",
    "                my_l = np.hstack([Xi, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([Xi, d])\n",
    "                solver.setup(\n",
    "                    q=c,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "                sols = np.array(Y)\n",
    "                parallel_time = total_time / len(X_np)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_QP_dataset(num_var, num_ineq, num_eq, num_examples):\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    b = np.random.uniform(-1, 1, size=(num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    # TODO: Can we keep h constant, if we are varying G?\n",
    "    d = np.sum(np.abs(G_base@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(1, num_var))\n",
    "        G_list.append(G_sample)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    problem = QPProblemVaryingG(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=b, d=d, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_b_d_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    B = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(num_varying_rows, num_var))\n",
    "        G_list.append(G_sample)\n",
    "    \n",
    "    # Create H matrix for each example\n",
    "    D_list = []\n",
    "    for Gi in G_list:\n",
    "        d = np.sum(np.abs(Gi @ np.linalg.pinv(A)), axis=1)  # Compute bounds for all inequalities\n",
    "        D_list.append(d)  # Resulting shape will be (num_ineq,)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    D = np.stack(D_list, axis=0)  # Shape (num_examples, num_ineq)\n",
    "    problem = QPProblemVaryingGbd(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=B, d=D, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_LP_dataset(num_var, num_ineq, num_eq, num_examples, scale='normal'):\n",
    "    if scale == 'normal':\n",
    "        obj_scale = 1\n",
    "        var_scale = 1\n",
    "        rhs_scale = 1\n",
    "    elif scale == 'large':\n",
    "        obj_scale = 1e9\n",
    "        var_scale = 1e3\n",
    "        rhs_scale = 1e3\n",
    "\n",
    "    np.random.seed(17)\n",
    "    c = np.random.uniform(-obj_scale, obj_scale, size=num_var)\n",
    "\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=var_scale, size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-rhs_scale, rhs_scale, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=var_scale, size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = ScaledLPProblem(Q, p, A, G, X, h, c)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    # with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        # pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_scaled_QP_problem(num_var, num_ineq, num_eq, num_examples, scale='normal'):\n",
    "    if scale == 'normal':\n",
    "        obj_scale = 1\n",
    "        var_scale = 1\n",
    "        rhs_scale = 1\n",
    "    elif scale == 'large':\n",
    "        obj_scale = 1e9\n",
    "        var_scale = 1e3\n",
    "        rhs_scale = 1e3\n",
    "\n",
    "    np.random.seed(17)\n",
    "\n",
    "    Q = np.diag(np.random.random(num_var)) * obj_scale\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=var_scale, size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-rhs_scale, rhs_scale, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=var_scale, size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    # with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        # pickle.dump(problem, f)\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "100\n",
      "tensor(1.4042e+08)\n",
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n",
      "Epoch 0 done. Time taken: 6.242254018783569. Rho: 0.5. Primal LR: 0.0001, Dual LR: 8.429431933839271e-05\n",
      "0: p-loss: 3.0885E+15, obj. val 2.8630E+15, Max eq.: 9.1313E+06, Max ineq.: 7.2171E+06, Mean eq.: 2.7898E+06, Mean ineq.: 1.3286E+06\n",
      "\n",
      "Epoch 1 done. Time taken: 6.377027988433838. Rho: 0.5. Primal LR: 9.605960100000001e-05, Dual LR: 5.309055429551135e-05\n",
      "1: p-loss: 3.0539E+15, obj. val 2.8299E+15, Max eq.: 9.0544E+06, Max ineq.: 7.1770E+06, Mean eq.: 2.7642E+06, Mean ineq.: 1.3424E+06\n",
      "\n",
      "Epoch 2 done. Time taken: 6.636106967926025. Rho: 0.5. Primal LR: 6.825545950103872e-05, Dual LR: 3.377544008989021e-05\n",
      "2: p-loss: 3.0476E+15, obj. val 2.8230E+15, Max eq.: 9.0396E+06, Max ineq.: 7.1687E+06, Mean eq.: 2.7645E+06, Mean ineq.: 1.3532E+06\n",
      "\n",
      "Epoch 3 done. Time taken: 6.408543109893799. Rho: 0.5. Primal LR: 4.475232137638109e-05, Dual LR: 2.1272570322901873e-05\n",
      "3: p-loss: 3.0465E+15, obj. val 2.8217E+15, Max eq.: 9.0380E+06, Max ineq.: 7.1713E+06, Mean eq.: 2.7658E+06, Mean ineq.: 1.3560E+06\n",
      "\n",
      "Epoch 4 done. Time taken: 6.447804927825928. Rho: 0.5. Primal LR: 2.847077732731955e-05, Dual LR: 1.3533300490703203e-05\n",
      "4: p-loss: 3.0461E+15, obj. val 2.8213E+15, Max eq.: 9.0374E+06, Max ineq.: 7.1712E+06, Mean eq.: 2.7659E+06, Mean ineq.: 1.3566E+06\n",
      "\n",
      "Epoch 5 done. Time taken: 5.942811012268066. Rho: 0.5. Primal LR: 1.7931568359471054e-05, Dual LR: 8.523592457219177e-06\n",
      "5: p-loss: 3.0460E+15, obj. val 2.8213E+15, Max eq.: 9.0377E+06, Max ineq.: 7.1712E+06, Mean eq.: 2.7659E+06, Mean ineq.: 1.3568E+06\n",
      "\n",
      "Epoch 6 done. Time taken: 7.173753023147583. Rho: 5.0. Primal LR: 1.1407803532657624e-05, Dual LR: 5.422585810406324e-06\n",
      "6: p-loss: 5.0302E+15, obj. val 2.8214E+15, Max eq.: 9.0378E+06, Max ineq.: 7.1711E+06, Mean eq.: 2.7657E+06, Mean ineq.: 1.3570E+06\n",
      "\n",
      "Epoch 7 done. Time taken: 6.0350730419158936. Rho: 5.0. Primal LR: 7.257479035344933e-06, Dual LR: 3.415272685621234e-06\n",
      "7: p-loss: 5.0301E+15, obj. val 2.8214E+15, Max eq.: 9.0380E+06, Max ineq.: 7.1710E+06, Mean eq.: 2.7656E+06, Mean ineq.: 1.3571E+06\n",
      "\n",
      "Epoch 8 done. Time taken: 6.250825881958008. Rho: 5.0. Primal LR: 4.570931799422277e-06, Dual LR: 2.172746913542606e-06\n",
      "8: p-loss: 5.0300E+15, obj. val 2.8215E+15, Max eq.: 9.0381E+06, Max ineq.: 7.1711E+06, Mean eq.: 2.7656E+06, Mean ineq.: 1.3572E+06\n",
      "\n",
      "Epoch 9 done. Time taken: 6.7451331615448. Rho: 50.0. Primal LR: 2.907960468580265e-06, Dual LR: 1.3684473507730198e-06\n",
      "9: p-loss: 2.4873E+16, obj. val 2.8215E+15, Max eq.: 9.0379E+06, Max ineq.: 7.1710E+06, Mean eq.: 2.7655E+06, Mean ineq.: 1.3572E+06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal scale LP test!\n",
    "num_var = 100\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 100\n",
    "SCALE = 'large'\n",
    "\n",
    "problem = create_scaled_QP_problem(num_var=num_var, num_ineq=num_ineq, num_eq=num_eq, num_examples=num_examples, scale=SCALE)\n",
    "\n",
    "print(problem.obj_fn(problem.Y).mean().item())\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "args = {\n",
    "        \"K\": 10,\n",
    "        \"L\": 500,\n",
    "        \"tau\": 0.8,\n",
    "        \"rho\": 0.5,\n",
    "        \"rho_max\": 5000,\n",
    "        \"alpha\": 10,\n",
    "        \"batch_size\": 200,\n",
    "        \"hidden_sizes\": [500, 500],\n",
    "        \"primal_lr\": 1e-4,\n",
    "        \"dual_lr\": 1e-4,\n",
    "        \"decay\": 0.99,\n",
    "        \"patience\": 10,\n",
    "        \"corrEps\": 1e-4,\n",
    "}\n",
    "\n",
    "run_name = \"test\"\n",
    "save_dir = os.path.join('outputs', 'QP_experiments',\n",
    "    run_name + \"-\" + str(time.time()).replace('.', '-'))\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'args.dict'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "# Run PDL\n",
    "trainer = PrimalDualTrainer(problem, args, save_dir, problem_type=\"Benchmark\", log=True)\n",
    "original_primal_net, original_dual_net, original_stats = trainer.train_PDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "1\n",
      "torch.Size([1, 100])\n",
      "torch.Size([50, 100])\n",
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "batch_size should be a positive integer value, but got batch_size=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(args, f)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run PDL\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPrimalDualTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_problem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBenchmark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m original_primal_net, original_dual_net, original_stats \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_PDL()\n",
      "File \u001b[0;32m~/Documents/CSE/MSc-AIT/Thesis/Code/masterthesis/primal_dual.py:125\u001b[0m, in \u001b[0;36mPrimalDualTrainer.__init__\u001b[0;34m(self, data, args, save_dir, problem_type, optimal_objective_train, optimal_objective_val, log)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# self.test_dataset = TensorDataset(self.data.testX.to(DEVICE), self.data.testX_scaled.to(DEVICE))\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# self.test_loader = DataLoader(self.test_dataset, batch_size=len(self.test_dataset))\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimal_net \u001b[38;5;241m=\u001b[39m PrimalNet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_sizes)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mDTYPE, device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/dataloader.py:382\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    378\u001b[0m             sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m batch_sampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;66;03m# auto_collation without custom batch_sampler\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m     batch_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mBatchSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_last \u001b[38;5;241m=\u001b[39m drop_last\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/sampler.py:323\u001b[0m, in \u001b[0;36mBatchSampler.__init__\u001b[0;34m(self, sampler, batch_size, drop_last)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    311\u001b[0m     sampler: Union[Sampler[\u001b[38;5;28mint\u001b[39m], Iterable[\u001b[38;5;28mint\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# is one way for an object to be an iterable, we don't do an `isinstance`\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# check here.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    322\u001b[0m     ):\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size should be a positive integer value, but got batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         )\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drop_last, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_last should be a boolean value, but got drop_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_last\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: batch_size should be a positive integer value, but got batch_size=0"
     ]
    }
   ],
   "source": [
    "num_var = 100\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 1\n",
    "original_problem = create_QP_dataset(num_var=num_var, num_ineq=num_ineq, num_eq=num_eq, num_examples=num_examples)\n",
    "print(original_problem.Y.shape)\n",
    "print(original_problem.G.shape)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "args = {\n",
    "        \"K\": 10,\n",
    "        \"L\": 500,\n",
    "        \"tau\": 0.8,\n",
    "        \"rho\": 0.5,\n",
    "        \"rho_max\": 5000,\n",
    "        \"alpha\": 10,\n",
    "        \"batch_size\": 200,\n",
    "        \"hidden_sizes\": [500, 500],\n",
    "        \"primal_lr\": 1e-4,\n",
    "        \"dual_lr\": 1e-4,\n",
    "        \"decay\": 0.99,\n",
    "        \"patience\": 10,\n",
    "        \"corrEps\": 1e-4,\n",
    "}\n",
    "\n",
    "run_name = \"test\"\n",
    "save_dir = os.path.join('outputs', 'QP_experiments',\n",
    "    run_name + \"-\" + str(time.time()).replace('.', '-'))\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'args.dict'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "# Run PDL\n",
    "trainer = PrimalDualTrainer(original_problem, args, save_dir, problem_type=\"Benchmark\", log=True)\n",
    "original_primal_net, original_dual_net, original_stats = trainer.train_PDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "100\n",
      "X dim: 50\n",
      "Y dim: 10\n",
      "Size of mu: 5\n",
      "Size of lambda: 5\n",
      "Epoch 0 done. Time taken: 7.658443212509155. Rho: 0.5. Primal LR: 7.249803359578537e-05, Dual LR: 6.491026283684025e-05\n",
      "0: p-loss: 2.4716E+00, obj. val -3.9875E+00, Max eq.: 2.8629E+00, Max ineq.: 5.7728E-01, Mean eq.: 1.3132E+00, Mean ineq.: 4.4015E-01\n",
      "\n",
      "Epoch 1 done. Time taken: 7.1610798835754395. Rho: 0.5. Primal LR: 4.6122196741809574e-05, Dual LR: 4.1294967113388845e-05\n",
      "1: p-loss: 1.9140E+00, obj. val -2.7137E+00, Max eq.: 2.4776E+00, Max ineq.: 5.0813E-01, Mean eq.: 1.2119E+00, Mean ineq.: 3.7762E-01\n",
      "\n",
      "Epoch 2 done. Time taken: 7.317880868911743. Rho: 0.5. Primal LR: 2.9048849430996377e-05, Dual LR: 2.6008546137772605e-05\n",
      "2: p-loss: 1.9665E+00, obj. val -2.5046E+00, Max eq.: 2.4716E+00, Max ineq.: 5.0474E-01, Mean eq.: 1.2028E+00, Mean ineq.: 3.7423E-01\n",
      "\n",
      "Epoch 3 done. Time taken: 7.222394227981567. Rho: 0.5. Primal LR: 1.848045639485463e-05, Dual LR: 1.6546259566473476e-05\n",
      "3: p-loss: 2.0111E+00, obj. val -2.4902E+00, Max eq.: 2.4694E+00, Max ineq.: 5.1222E-01, Mean eq.: 1.2008E+00, Mean ineq.: 3.8171E-01\n",
      "\n",
      "Epoch 4 done. Time taken: 7.291041135787964. Rho: 0.5. Primal LR: 1.163942815290034e-05, Dual LR: 1.0526490184835907e-05\n",
      "4: p-loss: 2.0298E+00, obj. val -2.4930E+00, Max eq.: 2.4632E+00, Max ineq.: 5.1739E-01, Mean eq.: 1.1989E+00, Mean ineq.: 3.8688E-01\n",
      "\n",
      "Epoch 5 done. Time taken: 7.22140097618103. Rho: 5.0. Primal LR: 7.404835256958406e-06, Dual LR: 6.629832272038531e-06\n",
      "5: p-loss: 4.3506E+01, obj. val -2.4937E+00, Max eq.: 2.4608E+00, Max ineq.: 5.2474E-01, Mean eq.: 1.1967E+00, Mean ineq.: 3.9422E-01\n",
      "\n",
      "Epoch 6 done. Time taken: 7.0663979053497314. Rho: 5.0. Primal LR: 4.663740229999262e-06, Dual LR: 4.217803066508772e-06\n",
      "6: p-loss: 4.3709E+01, obj. val -2.4983E+00, Max eq.: 2.4606E+00, Max ineq.: 5.2824E-01, Mean eq.: 1.1969E+00, Mean ineq.: 3.9772E-01\n",
      "\n",
      "Epoch 7 done. Time taken: 9.272633075714111. Rho: 5.0. Primal LR: 2.9670038450977095e-06, Dual LR: 2.656472043048682e-06\n",
      "7: p-loss: 4.3850E+01, obj. val -2.5022E+00, Max eq.: 2.4596E+00, Max ineq.: 5.3045E-01, Mean eq.: 1.1966E+00, Mean ineq.: 3.9994E-01\n",
      "\n",
      "Epoch 8 done. Time taken: 7.584395170211792. Rho: 50.0. Primal LR: 1.868689135513392e-06, Dual LR: 1.6900089579220103e-06\n",
      "8: p-loss: 4.6143E+02, obj. val -2.5015E+00, Max eq.: 2.4587E+00, Max ineq.: 5.3081E-01, Mean eq.: 1.1962E+00, Mean ineq.: 4.0029E-01\n",
      "\n",
      "Epoch 9 done. Time taken: 6.436199903488159. Rho: 50.0. Primal LR: 1.188832905978862e-06, Dual LR: 1.0644075786444312e-06\n",
      "9: p-loss: 4.6166E+02, obj. val -2.5020E+00, Max eq.: 2.4584E+00, Max ineq.: 5.3121E-01, Mean eq.: 1.1961E+00, Mean ineq.: 4.0070E-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_var = 10\n",
    "num_ineq = 5\n",
    "num_eq = 5\n",
    "num_examples = 100\n",
    "num_varying_rows = 5\n",
    "modified_problem = create_varying_G_dataset(num_var=num_var, num_ineq=num_ineq, num_eq=num_eq, num_examples=num_examples, num_varying_rows=num_varying_rows)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "args = {\n",
    "        \"K\": 10,\n",
    "        \"L\": 500,\n",
    "        \"tau\": 0.8,\n",
    "        \"rho\": 0.5,\n",
    "        \"rho_max\": 5000,\n",
    "        \"alpha\": 10,\n",
    "        \"batch_size\": 200,\n",
    "        \"hidden_sizes\": [500, 500],\n",
    "        \"primal_lr\": 1e-4,\n",
    "        \"dual_lr\": 1e-4,\n",
    "        \"decay\": 0.99,\n",
    "        \"patience\": 10,\n",
    "        \"corrEps\": 1e-4,\n",
    "}\n",
    "\n",
    "run_name = \"test\"\n",
    "save_dir = os.path.join('outputs', 'QP_experiments',\n",
    "    run_name + \"-\" + str(time.time()).replace('.', '-'))\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'args.dict'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "# Run PDL\n",
    "trainer = PrimalDualTrainer(modified_problem, args, save_dir, problem_type=\"Benchmark\", log=True)\n",
    "modified_primal_net, modified_dual_net, modified_stats = trainer.train_PDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "Original problem -- opt: -18.500195218984732, pred: -18.450353886636613\n",
      "Modified problem -- opt: -17.31410062843396, pred: -17.119726465504737\n",
      "torch.Size([80, 50])\n",
      "torch.Size([80, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/jbnxb2792bx_bmgd675jx1fh0000gq/T/ipykernel_83511/3526701426.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  opt_obj_values = modified_problem.obj_fn(torch.tensor(modified_problem.Y[:80]))\n"
     ]
    }
   ],
   "source": [
    "opt_Y = torch.tensor(original_problem.calc_Y())\n",
    "opt_obj_values = original_problem.obj_fn(opt_Y)\n",
    "predicted_Y = original_primal_net(original_problem.trainX)\n",
    "predicted_obj_values = original_problem.obj_fn(predicted_Y)\n",
    "\n",
    "print(f\"Original problem -- opt: {opt_obj_values.mean()}, pred: {predicted_obj_values.mean()}\")\n",
    "\n",
    "\n",
    "opt_obj_values = modified_problem.obj_fn(torch.tensor(modified_problem.Y[:80]))\n",
    "predicted_Y = modified_primal_net(modified_problem.trainX)\n",
    "predicted_obj_values = original_problem.obj_fn(predicted_Y)\n",
    "\n",
    "print(f\"Modified problem -- opt: {opt_obj_values.mean()}, pred: {predicted_obj_values.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "100\n",
      "X dim: 600\n",
      "Y dim: 50\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run PDL\u001b[39;00m\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PrimalDualTrainer(modified_problem, args, save_dir, problem_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 36\u001b[0m modified_primal_net, modified_dual_net, modified_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_PDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CSE/MSc-AIT/Thesis/Code/masterthesis/primal_dual.py:332\u001b[0m, in \u001b[0;36mPrimalDualTrainer.train_PDL\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Xvalid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_loader:\n\u001b[1;32m    331\u001b[0m     y \u001b[38;5;241m=\u001b[39m frozen_primal_net(Xvalid[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 332\u001b[0m     mu_valid, lamb_valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXvalid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m     mu_k_valid, lamb_k_valid \u001b[38;5;241m=\u001b[39m frozen_dual_net(Xvalid[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    334\u001b[0m     curr_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_loss(Xvalid[\u001b[38;5;241m0\u001b[39m], y, mu_valid, lamb_valid, mu_k_valid, lamb_k_valid, rhs_eq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rhs_ineq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, targets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/CSE/MSc-AIT/Thesis/Code/masterthesis/primal_dual.py:831\u001b[0m, in \u001b[0;36mDualNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 831\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m     out_mu \u001b[38;5;241m=\u001b[39m out[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mu_size]\n\u001b[1;32m    833\u001b[0m     out_lamb \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mu_size:]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_var = 50\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 100\n",
    "num_varying_rows = 10\n",
    "modified_problem = create_varying_G_b_d_dataset(num_var=num_var, num_ineq=num_ineq, num_eq=num_eq, num_examples=num_examples, num_varying_rows=num_varying_rows)\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "args = {\n",
    "        \"K\": 10,\n",
    "        \"L\": 500,\n",
    "        \"tau\": 0.8,\n",
    "        \"rho\": 0.5,\n",
    "        \"rho_max\": 5000,\n",
    "        \"alpha\": 10,\n",
    "        \"batch_size\": 200,\n",
    "        \"hidden_sizes\": [500, 500],\n",
    "        \"primal_lr\": 1e-4,\n",
    "        \"dual_lr\": 1e-4,\n",
    "        \"decay\": 0.99,\n",
    "        \"patience\": 10,\n",
    "        \"corrEps\": 1e-4,\n",
    "}\n",
    "\n",
    "run_name = \"test\"\n",
    "save_dir = os.path.join('outputs', 'QP_experiments',\n",
    "    run_name + \"-\" + str(time.time()).replace('.', '-'))\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'args.dict'), 'wb') as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "# Run PDL\n",
    "trainer = PrimalDualTrainer(modified_problem, args, save_dir, problem_type=\"Benchmark\", log=True)\n",
    "modified_primal_net, modified_dual_net, modified_stats = trainer.train_PDL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified problem -- opt: -17.933158508477426, pred: -17.543311041351224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/jbnxb2792bx_bmgd675jx1fh0000gq/T/ipykernel_21687/1671212256.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  opt_obj_values = modified_problem.obj_fn(torch.tensor(modified_problem.Y[:80]))\n"
     ]
    }
   ],
   "source": [
    "opt_obj_values = modified_problem.obj_fn(torch.tensor(modified_problem.Y[:80]))\n",
    "predicted_obj_values = modified_problem.obj_fn(modified_primal_net(modified_problem.trainX))\n",
    "\n",
    "print(f\"Modified problem -- opt: {opt_obj_values.mean()}, pred: {predicted_obj_values.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

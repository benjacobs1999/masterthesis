{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.autograd.profiler as profiler\n",
    "import numpy as np\n",
    "\n",
    "from QP_problem import SimpleProblem, OriginalQPProblem, QPProblemVaryingG, QPProblemVaryingGbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_QP_dataset(num_var, num_ineq, num_eq, num_examples):\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    b = np.random.uniform(-1, 1, size=(num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    # TODO: Can we keep h constant, if we are varying G?\n",
    "    d = np.sum(np.abs(G_base@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(1, num_var))\n",
    "        G_list.append(G_sample)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    problem = QPProblemVaryingG(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=b, d=d, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_b_d_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    B = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(num_varying_rows, num_var))\n",
    "        G_list.append(G_sample)\n",
    "    \n",
    "    # Create H matrix for each example\n",
    "    D_list = []\n",
    "    for Gi in G_list:\n",
    "        d = np.sum(np.abs(Gi @ np.linalg.pinv(A)), axis=1)  # Compute bounds for all inequalities\n",
    "        D_list.append(d)  # Resulting shape will be (num_ineq,)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    D = np.stack(D_list, axis=0)  # Shape (num_examples, num_ineq)\n",
    "    problem = QPProblemVaryingGbd(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=B, d=D, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_scaled_QP_problem(num_var, num_ineq, num_eq, num_examples, scale='normal'):\n",
    "    if scale == 'normal':\n",
    "        obj_scale = 1\n",
    "        var_scale = 1\n",
    "        rhs_scale = 1\n",
    "    elif scale == 'large':\n",
    "        obj_scale = 1e9\n",
    "        var_scale = 1e3\n",
    "        rhs_scale = 1e3\n",
    "\n",
    "    np.random.seed(17)\n",
    "\n",
    "    Q = np.diag(np.random.random(num_var)) * obj_scale\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=var_scale, size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-rhs_scale, rhs_scale, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=var_scale, size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    # with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        # pickle.dump(problem, f)\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "DTYPE = torch.float64\n",
    "DEVICE = torch.device=\"cpu\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(42)\n",
    "print(f\"Running on {DEVICE}\")\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, eq_cm, ineq_cm, eq_rhs, ineq_rhs):\n",
    "        self.x = x\n",
    "        self.eq_cm = eq_cm \n",
    "        self.ineq_cm = ineq_cm\n",
    "        self.eq_rhs = eq_rhs\n",
    "        self.ineq_rhs = ineq_rhs\n",
    "        self._index = 0  # Internal index for tracking iteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple of input and target for the given index\n",
    "        #! Change per data set.\n",
    "        # return self.x[idx], self.eq_cm[idx], self.ineq_cm[idx], self.eq_rhs[idx], self.ineq_rhs[idx]\n",
    "        return self.x[idx]\n",
    "\n",
    "        \n",
    "class PrimalDualTrainer():\n",
    "\n",
    "    def __init__(self, data, args, save_dir):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            data (_type_): _description_\n",
    "            args (_type_): _description_\n",
    "            save_dir (_type_): _description_\n",
    "            problem_type (str, optional): Either \"GEP\" or \"Benchmark\". Defaults to \"GEP\".\n",
    "            log (bool, optional): _description_. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"X dim: {data.xdim}\")\n",
    "        print(f\"Y dim: {data.ydim}\")\n",
    "\n",
    "        print(f\"Size of mu: {data.nineq}\")\n",
    "        print(f\"Size of lambda: {data.neq}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.outer_iterations = args[\"outer_iterations\"]\n",
    "        self.inner_iterations = args[\"inner_iterations\"]\n",
    "        self.tau = args[\"tau\"]\n",
    "        self.rho = args[\"rho\"]\n",
    "        self.rho_max = args[\"rho_max\"]\n",
    "        self.alpha = args[\"alpha\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.hidden_sizes = args[\"hidden_sizes\"]\n",
    "\n",
    "        self.primal_lr = args[\"primal_lr\"]\n",
    "        self.dual_lr = args[\"dual_lr\"]\n",
    "        self.decay = args[\"decay\"]\n",
    "        self.patience = args[\"patience\"]\n",
    "        \n",
    "        # for logging\n",
    "        self.step = 0\n",
    "\n",
    "        X = data.X\n",
    "        eq_cm = data.eq_cm\n",
    "        ineq_cm = data.ineq_cm\n",
    "        eq_rhs = data.eq_rhs\n",
    "        ineq_rhs = data.ineq_rhs\n",
    "\n",
    "        train = data.train_indices\n",
    "        valid = data.valid_indices\n",
    "        test = data.test_indices\n",
    "\n",
    "        # Traning data in a data set\n",
    "        #! Vary per experiment\n",
    "        # self.train_dataset = CustomDataset(X[train].to(DEVICE), eq_cm[train], ineq_cm[train], eq_rhs[train], ineq_rhs[train])\n",
    "        self.train_dataset = CustomDataset(X[train].to(DEVICE), eq_cm, ineq_cm, eq_rhs[train], ineq_rhs)\n",
    "        self.valid_dataset = CustomDataset(X[valid].to(DEVICE), eq_cm, ineq_cm, eq_rhs[valid], ineq_rhs)\n",
    "        self.test_dataset = CustomDataset(X[test].to(DEVICE), eq_cm, ineq_cm, eq_rhs[test], ineq_rhs)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=1000, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        self.primal_net = PrimalNet(self.data, self.hidden_sizes).to(dtype=DTYPE, device=DEVICE)\n",
    "        self.dual_net = DualNet(self.data, self.hidden_sizes, self.data.nineq, self.data.neq).to(dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "        self.primal_optim = torch.optim.Adam(self.primal_net.parameters(), lr=self.primal_lr)\n",
    "        self.dual_optim = torch.optim.Adam(self.dual_net.parameters(), lr=self.dual_lr)\n",
    "\n",
    "        # Add schedulers\n",
    "        self.primal_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.primal_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "        self.dual_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.dual_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "\n",
    "    def train_PDL(self,):\n",
    "        try:\n",
    "            prev_v_k = 0\n",
    "            for k in range(self.outer_iterations):\n",
    "                begin_time = time.time()\n",
    "                epoch_stats = {}\n",
    "                frozen_dual_net = copy.deepcopy(self.dual_net)\n",
    "                # self.logger.log_rho_vk(self.rho, prev_v_k, self.step)\n",
    "                for l1 in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update primal net using primal loss\n",
    "                    self.primal_net.train()\n",
    "\n",
    "                    # Accumulate training loss over all batches\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        self.primal_optim.zero_grad()\n",
    "                        y = self.primal_net(Xtrain, Xtrain, self.train_dataset.ineq_rhs)\n",
    "                        with torch.no_grad():\n",
    "                            mu, lamb = frozen_dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                        batch_loss = self.primal_loss(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, Xtrain, self.train_dataset.ineq_rhs, mu, lamb).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.primal_optim.step()\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    with torch.no_grad():\n",
    "                        self.primal_net.eval()\n",
    "                        frozen_dual_net.eval()\n",
    "                        val_loss = 0\n",
    "                        for Xvalid in self.valid_loader:\n",
    "                            # for Xvalid, valid_eq_cm, valid_ineq_cm, valid_eq_rhs, valid_ineq_rhs in self.valid_loader:\n",
    "                            y = self.primal_net(Xvalid, Xvalid, self.valid_dataset.ineq_rhs)\n",
    "                            mu, lamb = frozen_dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            val_loss += self.primal_loss(y, self.valid_dataset.eq_cm, self.valid_dataset.ineq_cm, Xvalid, self.valid_dataset.ineq_rhs, mu, lamb).sum()\n",
    "                        val_loss /= len(self.valid_loader)\n",
    "                        # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                        self.primal_scheduler.step(torch.sign(val_loss) * (torch.abs(val_loss) / self.rho))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Copy primal net into frozen primal net\n",
    "                    frozen_primal_net = copy.deepcopy(self.primal_net)\n",
    "\n",
    "                    # Calculate v_k\n",
    "                    y = frozen_primal_net(self.train_dataset.x, self.train_dataset.eq_rhs, self.train_dataset.ineq_rhs)\n",
    "                    mu_k, lamb_k = frozen_dual_net(self.train_dataset.x, self.train_dataset.eq_cm)\n",
    "                    v_k = self.violation(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, self.train_dataset.eq_rhs, self.train_dataset.ineq_rhs, mu_k)\n",
    "\n",
    "                for l in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update dual net using dual loss\n",
    "                    self.dual_net.train()\n",
    "                    frozen_primal_net.train()\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        self.dual_optim.zero_grad()\n",
    "                        mu, lamb = self.dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                        with torch.no_grad():\n",
    "                            mu_k, lamb_k = frozen_dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                            y = frozen_primal_net(Xtrain, Xtrain, self.train_dataset.ineq_rhs)\n",
    "                        # ! Test other loss!\n",
    "                        batch_loss = self.dual_loss(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, Xtrain, self.train_dataset.ineq_rhs, mu, lamb, mu_k, lamb_k).mean()\n",
    "                        # batch_loss = self.dual_loss_changed(y, train_eq_cm, train_ineq_cm, train_eq_rhs, train_ineq_rhs, mu, lamb, mu_k, lamb_k).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.dual_optim.step()\n",
    "                    \n",
    "                    # with torch.no_grad():\n",
    "                    #     # Logg training loss:\n",
    "                    #     self.logger.log_loss(batch_loss, \"dual\", self.step)\n",
    "                    #     self.logger.log_train(self.data, primal_net=frozen_primal_net, dual_net=self.dual_net, step=self.step)\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    # TODO! Does scheduler correctly decrease LR when rho is increased, if the training set is small?\n",
    "                    with torch.no_grad():\n",
    "                        frozen_primal_net.eval()\n",
    "                        self.dual_net.eval()\n",
    "                        val_loss = 0\n",
    "                        for Xvalid in self.valid_loader:\n",
    "                            y = frozen_primal_net(Xvalid, Xvalid, self.valid_dataset.ineq_rhs)\n",
    "                            mu_valid, lamb_valid = self.dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            mu_k_valid, lamb_k_valid = frozen_dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            val_loss += self.dual_loss(y, self.valid_dataset.eq_cm, self.valid_dataset.ineq_cm, Xvalid, self.valid_dataset.ineq_rhs, mu_valid, lamb_valid, mu_k_valid, lamb_k_valid).sum()\n",
    "                        val_loss /= len(self.valid_loader)\n",
    "                    # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                    self.dual_scheduler.step(torch.sign(val_loss) * (torch.abs(val_loss) / self.rho))\n",
    "\n",
    "                end_time = time.time()\n",
    "                stats = epoch_stats\n",
    "                print(\"-\"*40)\n",
    "                print(f\"Epoch {k} done. Time taken: {end_time - begin_time}. Rho: {self.rho}. Primal LR: {self.primal_optim.param_groups[0]['lr']}, Dual LR: {self.dual_optim.param_groups[0]['lr']}\")\n",
    "\n",
    "                # Update rho from the second iteration onward.\n",
    "                if k > 0 and v_k > self.tau * prev_v_k:\n",
    "                    self.rho = np.min([self.alpha * self.rho, self.rho_max])\n",
    "\n",
    "                prev_v_k = v_k\n",
    "            \n",
    "                print(f\"Validation set evaluate:\")\n",
    "                with torch.no_grad():\n",
    "                    self.primal_net.eval()\n",
    "                    self.dual_net.eval()\n",
    "                    self.evaluate(self.valid_loader, self.valid_dataset, self.primal_net, self.dual_net)\n",
    "                    \n",
    "            print(\"-\"*40)\n",
    "            print(f\"Test set evaluate:\")\n",
    "            with torch.no_grad():\n",
    "                self.primal_net.eval()\n",
    "                self.dual_net.eval()\n",
    "                self.evaluate(self.test_loader, self.test_dataset, self.primal_net, self.dual_net)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e, flush=True)\n",
    "            # # Ensure writer is closed even if an exception occurs\n",
    "            # if self.logger:\n",
    "            #     self.logger.close()\n",
    "            raise\n",
    "\n",
    "        with open(os.path.join(self.save_dir, 'stats.dict'), 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "        with open(os.path.join(self.save_dir, 'primal_net.dict'), 'wb') as f:\n",
    "            torch.save(self.primal_net.state_dict(), f)\n",
    "        with open(os.path.join(self.save_dir, 'dual_net.dict'), 'wb') as f:\n",
    "            torch.save(self.dual_net.state_dict(), f)\n",
    "\n",
    "        return self.primal_net, self.dual_net, stats\n",
    "\n",
    "    def evaluate(self, loader, dataset, primal_net, dual_net):        \n",
    "        obj_values = []\n",
    "        primal_losses = []\n",
    "        ineq_max_vals = []\n",
    "        ineq_mean_vals = []\n",
    "        eq_max_vals = []\n",
    "        eq_mean_vals = []\n",
    "\n",
    "        for X in loader:\n",
    "\n",
    "            # Forward pass through networks\n",
    "            Y = primal_net(X, dataset.eq_rhs, dataset.ineq_rhs)\n",
    "            mu, lamb = dual_net(X, dataset.eq_cm)\n",
    "\n",
    "            # Compute and store metrics\n",
    "            obj_values.append(self.data.obj_fn(Y).detach().cpu().numpy())\n",
    "            primal_losses.append(self.primal_loss(Y, dataset.eq_cm, dataset.ineq_cm, X, dataset.ineq_rhs, mu, lamb).detach().cpu().numpy())\n",
    "            ineq_dist = self.data.ineq_dist(Y, dataset.ineq_cm, dataset.ineq_rhs)\n",
    "            eq_resid = self.data.eq_resid(Y, dataset.eq_cm, X)\n",
    "\n",
    "            ineq_max_vals.append(torch.max(ineq_dist, dim=1)[0].detach().cpu().numpy())\n",
    "            ineq_mean_vals.append(torch.mean(ineq_dist, dim=1).detach().cpu().numpy())\n",
    "            eq_max_vals.append(torch.max(torch.abs(eq_resid), dim=1)[0].detach().cpu().numpy())\n",
    "            eq_mean_vals.append(torch.mean(torch.abs(eq_resid), dim=1).detach().cpu().numpy())\n",
    "\n",
    "        # Convert lists to arrays for easier handling\n",
    "        obj_values = np.concatenate(obj_values)\n",
    "        primal_losses = np.concatenate(primal_losses)\n",
    "        ineq_max_vals = np.concatenate(ineq_max_vals)\n",
    "        ineq_mean_vals = np.concatenate(ineq_mean_vals)\n",
    "        eq_max_vals = np.concatenate(eq_max_vals)\n",
    "        eq_mean_vals = np.concatenate(eq_mean_vals)\n",
    "\n",
    "        # Print aggregated statistics\n",
    "        print(f\"Obj: {np.mean(obj_values)}\")\n",
    "        print(f\"Primal Loss: {np.mean(primal_losses)}\")\n",
    "        print(f\"Ineq max: {np.mean(ineq_max_vals)}\")\n",
    "        print(f\"Ineq mean: {np.mean(ineq_mean_vals)}\")\n",
    "        print(f\"Eq max: {np.mean(eq_max_vals)}\")\n",
    "        print(f\"Eq mean: {np.mean(eq_mean_vals)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def primal_loss(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb):\n",
    "        obj = self.data.obj_fn(y)\n",
    "        \n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs)\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)\n",
    "\n",
    "        # ! Clamp mu?\n",
    "        # Element-wise clamping of mu_i when g_i (ineq) is negative\n",
    "        # mu = torch.where(ineq < 0, torch.zeros_like(mu), mu)\n",
    "        # ! Clamp ineq_resid?\n",
    "        # ineq = ineq.clamp(min=0)\n",
    "\n",
    "        lagrange_ineq = torch.sum(mu * ineq, dim=1)  # Shape (batch_size,)\n",
    "\n",
    "        lagrange_eq = torch.sum(lamb * eq, dim=1)   # Shape (batch_size,)\n",
    "\n",
    "        violation_ineq = torch.sum(torch.maximum(ineq, torch.zeros_like(ineq)) ** 2, dim=1)\n",
    "        violation_eq = torch.sum(eq ** 2, dim=1)\n",
    "        penalty = self.rho/2 * (violation_ineq + violation_eq)\n",
    "\n",
    "        loss = (obj + (lagrange_ineq + lagrange_eq + penalty))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def dual_loss(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb, mu_k, lamb_k):\n",
    "        # mu = [batch, g]\n",
    "        # lamb = [batch, h]\n",
    "\n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs) # [batch, g]\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)   # [batch, h]\n",
    "\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        target_mu = torch.maximum(mu_k + self.rho * ineq, torch.zeros_like(ineq))\n",
    "        # target_mu = torch.maximum(mu_k + 1e-1 * ineq, torch.zeros_like(ineq))\n",
    "\n",
    "        dual_resid_ineq = mu - target_mu # [batch, g]\n",
    "\n",
    "        dual_resid_ineq = torch.norm(dual_resid_ineq, dim=1)  # [batch]\n",
    "\n",
    "        # Compute the dual residuals for equality constraints\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        dual_resid_eq = lamb - (lamb_k + self.rho * eq)\n",
    "        # dual_resid_eq = lamb - (lamb_k + 1e-1 * eq)\n",
    "        dual_resid_eq = torch.norm(dual_resid_eq, dim=1)  # Norm along constraint dimension\n",
    "\n",
    "        loss = (dual_resid_ineq + dual_resid_eq)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def dual_loss_changed(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb, mu_k, lamb_k):\n",
    "        #! We maximize the dual obj func, so to use it in the loss, take the negation.\n",
    "        dual_obj = -self.data.dual_obj_fn(eq_rhs, ineq_rhs, mu, lamb)\n",
    "\n",
    "        #! Enforced with ReLU.\n",
    "        # ineq = self.data.dual_ineq_resid(mu, lamb)\n",
    "\n",
    "        eq = self.data.dual_eq_resid(mu, lamb, eq_cm, ineq_cm)\n",
    "        # Lagrange multiplier becomes y\n",
    "        lagrange_eq = torch.sum(y * eq, dim=1)\n",
    "\n",
    "        violation_eq = torch.sum(eq ** 2, dim=1)\n",
    "\n",
    "        penalty = self.rho/2 * violation_eq\n",
    "\n",
    "        loss = dual_obj + lagrange_eq + penalty\n",
    "        # loss = dual_obj + penalty\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def violation(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu_k):\n",
    "        # Calculate the equality constraint function h_x(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)  # Assume shape (num_samples, n_eq)\n",
    "        \n",
    "        # Calculate the infinity norm of h_x(y)\n",
    "        eq_inf_norm = torch.abs(eq).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Calculate the inequality constraint function g_x(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs)  # Assume shape (num_samples, n_ineq)\n",
    "        \n",
    "        # Calculate sigma_x(y) for each inequality constraint\n",
    "        sigma_y = torch.maximum(ineq, -mu_k / self.rho)  # Element-wise max\n",
    "        \n",
    "        # Calculate the infinity norm of sigma_x(y)\n",
    "        sigma_y_inf_norm = torch.abs(sigma_y).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Compute v_k as the maximum of the two norms\n",
    "        v_k = torch.maximum(eq_inf_norm, sigma_y_inf_norm)  # Shape: (num_samples,)\n",
    "        \n",
    "        return v_k.max().item()\n",
    "\n",
    "class PrimalNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        \n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes + [data.ydim]\n",
    "        layers = []\n",
    "\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            if out_size != data.ydim:  # Add ReLU activation for hidden layers only\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, eq_rhs, ineq_rhs):\n",
    "        return self.net(x)\n",
    "\n",
    "class DualNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes, mu_size, lamb_size):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        self._mu_size = mu_size\n",
    "        self._lamb_size = lamb_size\n",
    "\n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes\n",
    "        # layer_sizes = [2*data.xdim + 1000] + self._hidden_sizes\n",
    "        layers = []\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        # Add the output layer\n",
    "        self.out_layer = nn.Linear(self._hidden_sizes[-1], self._mu_size + self._lamb_size)\n",
    "        nn.init.zeros_(self.out_layer.weight)  # Initialize output layer weights to 0\n",
    "        nn.init.zeros_(self.out_layer.bias)    # Initialize output layer biases to 0\n",
    "        layers.append(self.out_layer)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, *args):\n",
    "        out = self.net(x)\n",
    "        #! ReLU to enforce nonnegativity in mu. Test with it.\n",
    "        #! Does this work with zero initialization?\n",
    "        # out_mu = torch.relu(out[:, :self._mu_size])\n",
    "        out_mu = out[:, :self._mu_size]\n",
    "        out_lamb = out[:, self._mu_size:]\n",
    "        return out_mu, out_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "num_var = 100\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 10000\n",
    "\n",
    "save_dir = \"benchmark_experiment_output\"\n",
    "\n",
    "data = create_QP_dataset(num_var, num_ineq, num_eq, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PrimalDualTrainer(data, args, save_dir)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# with profiler.profile(record_shapes=True) as prof:\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m primal_net, dual_net, stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_PDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 173\u001b[0m, in \u001b[0;36mPrimalDualTrainer.train_PDL\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_loss(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39meq_cm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mineq_cm, Xtrain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mineq_rhs, mu, lamb, mu_k, lamb_k)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# batch_loss = self.dual_loss_changed(y, train_eq_cm, train_ineq_cm, train_eq_rhs, train_ineq_rhs, mu, lamb, mu_k, lamb_k).mean()\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    175\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"outer_iterations\": 10,\n",
    "    \"inner_iterations\": 500,\n",
    "    \"tau\": 0.8,\n",
    "    \"rho\": 0.5,\n",
    "    \"rho_max\": 5000,\n",
    "    \"alpha\": 10,\n",
    "    \"batch_size\": 100,\n",
    "    \"hidden_sizes\": [500, 500],\n",
    "    \"primal_lr\": 1e-4,\n",
    "    \"dual_lr\": 1e-4,\n",
    "    \"decay\": 0.99,\n",
    "    \"patience\": 10,\n",
    "    \"corrEps\": 1e-4,\n",
    "    \"train\": 0.8,\n",
    "    \"valid\": 0.1,\n",
    "    \"test\": 0.1\n",
    "}\n",
    "\n",
    "trainer = PrimalDualTrainer(data, args, save_dir)\n",
    "# with profiler.profile(record_shapes=True) as prof:\n",
    "primal_net, dual_net, stats = trainer.train_PDL()\n",
    "    \n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

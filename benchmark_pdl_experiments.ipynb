{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.autograd.profiler as profiler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import osqp\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "import time\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "class SimpleProblem(ABC):\n",
    "    \"\"\"\n",
    "    minimize_y 1/2 * y^T Q y + p^Ty\n",
    "    s.t.       Ay =  b\n",
    "               Gy <= d\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Q, p, A, G, b, d, valid_frac=0.0833, test_frac=0.0833):\n",
    "        self._Q = torch.tensor(Q)\n",
    "        self._p = torch.tensor(p)\n",
    "        self._A = torch.tensor(A)\n",
    "        self._G = torch.tensor(G)\n",
    "        self._b = torch.tensor(b) # equality RHS\n",
    "        self._d = torch.tensor(d) # inequality RHS\n",
    "\n",
    "        self._eq_cm = self._A\n",
    "        self._ineq_cm = self._G\n",
    "        self._eq_rhs = self._b\n",
    "        self._ineq_rhs = self._d\n",
    "\n",
    "        self._valid_frac = valid_frac\n",
    "        self._test_frac = test_frac\n",
    "\n",
    "        self._Y = None\n",
    "        self._ydim = Q.shape[0]\n",
    "\n",
    "        ### For Pytorch\n",
    "        self._device = None\n",
    "\n",
    "        #! Implement in child!\n",
    "        self._X = None\n",
    "        self._num = None\n",
    "        self._neq = None\n",
    "        self._nineq = None\n",
    "        self._xdim = None\n",
    "\n",
    "\n",
    "    ##### ABSTRACT METHODS #####\n",
    "\n",
    "    @abstractmethod\n",
    "    def eq_resid(self, X, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def ineq_resid(self, X, Y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def calc_Y(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"SimpleProblem-{}-{}-{}-{}\".format(\n",
    "            str(self.ydim), str(self.nineq), str(self.neq), str(self.num)\n",
    "        )\n",
    "    \n",
    "    ##### REG METHODS #####\n",
    "\n",
    "    @property\n",
    "    def eq_cm(self):\n",
    "        return self._eq_cm\n",
    "\n",
    "    @property\n",
    "    def ineq_cm(self):\n",
    "        return self._ineq_cm\n",
    "    \n",
    "    @property\n",
    "    def eq_rhs(self):\n",
    "        return self._eq_rhs\n",
    "    \n",
    "    @property\n",
    "    def ineq_rhs(self):\n",
    "        return self._ineq_rhs\n",
    "\n",
    "    @property\n",
    "    def Q(self):\n",
    "        return self._Q\n",
    "\n",
    "    @property\n",
    "    def p(self):\n",
    "        return self._p\n",
    "\n",
    "    @property\n",
    "    def A(self):\n",
    "        return self._A\n",
    "\n",
    "    @property\n",
    "    def G(self):\n",
    "        return self._G\n",
    "\n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b\n",
    "\n",
    "    @property\n",
    "    def d(self):\n",
    "        return self._d\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "\n",
    "    @property\n",
    "    def Y(self):\n",
    "        return self._Y\n",
    "\n",
    "    @property\n",
    "    def partial_vars(self):\n",
    "        return self._partial_vars\n",
    "\n",
    "    @property\n",
    "    def other_vars(self):\n",
    "        return self._other_vars\n",
    "\n",
    "    @property\n",
    "    def partial_unknown_vars(self):\n",
    "        return self._partial_vars\n",
    "\n",
    "    @property\n",
    "    def Q_np(self):\n",
    "        return self.Q.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def p_np(self):\n",
    "        return self.p.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def A_np(self):\n",
    "        return self.A.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def G_np(self):\n",
    "        return self.G.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def b_np(self):\n",
    "        return self.b.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def d_np(self):\n",
    "        return self.d.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def X_np(self):\n",
    "        return self.X.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def Y_np(self):\n",
    "        return self.Y.detach().cpu().numpy()\n",
    "\n",
    "    @property\n",
    "    def xdim(self):\n",
    "        return self._xdim\n",
    "\n",
    "    @property\n",
    "    def ydim(self):\n",
    "        return self._ydim\n",
    "\n",
    "    @property\n",
    "    def num(self):\n",
    "        return self._num\n",
    "\n",
    "    @property\n",
    "    def neq(self):\n",
    "        return self._neq\n",
    "\n",
    "    @property\n",
    "    def nineq(self):\n",
    "        return self._nineq\n",
    "\n",
    "    @property\n",
    "    def nknowns(self):\n",
    "        return self._nknowns\n",
    "\n",
    "    @property\n",
    "    def valid_frac(self):\n",
    "        return self._valid_frac\n",
    "\n",
    "    @property\n",
    "    def test_frac(self):\n",
    "        return self._test_frac\n",
    "\n",
    "    @property\n",
    "    def train_frac(self):\n",
    "        return 1 - self.valid_frac - self.test_frac\n",
    "    \n",
    "    @property\n",
    "    def train_indices(self):\n",
    "        return list(range(int(self.num * self.train_frac)))\n",
    "    \n",
    "    @property\n",
    "    def valid_indices(self):\n",
    "        return list(range(int(self.num * self.train_frac), int(self.num * (self.train_frac + self.valid_frac))))\n",
    "    \n",
    "    @property\n",
    "    def test_indices(self):\n",
    "        return list(range(int(self.num * (self.train_frac + self.valid_frac)), self.num))\n",
    "\n",
    "    @property\n",
    "    def trainX(self):\n",
    "        return self.X[: int(self.num * self.train_frac)]\n",
    "\n",
    "    @property\n",
    "    def validX(self):\n",
    "        return self.X[\n",
    "            int(self.num * self.train_frac) : int(\n",
    "                self.num * (self.train_frac + self.valid_frac)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def testX(self):\n",
    "        return self.X[int(self.num * (self.train_frac + self.valid_frac)) :]\n",
    "\n",
    "    @property\n",
    "    def trainY(self):\n",
    "        return self.Y[: int(self.num * self.train_frac)]\n",
    "\n",
    "    @property\n",
    "    def validY(self):\n",
    "        return self.Y[\n",
    "            int(self.num * self.train_frac) : int(\n",
    "                self.num * (self.train_frac + self.valid_frac)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def testY(self):\n",
    "        return self.Y[int(self.num * (self.train_frac + self.valid_frac)) :]\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "\n",
    "    def obj_fn(self, Y):\n",
    "        return (0.5 * (Y @ self.Q) * Y +  self.p * Y).sum(dim=1)\n",
    "\n",
    "    def ineq_dist(self, X, Y):\n",
    "        resids = self.ineq_resid(X, Y)\n",
    "        return torch.clamp(resids, 0)\n",
    "\n",
    "    def eq_grad(self, X, Y):\n",
    "        return 2 * (Y @ self.A.T - X) @ self.A\n",
    "\n",
    "    def ineq_grad(self, X, Y):\n",
    "        ineq_dist = self.ineq_dist(X, Y)\n",
    "        return 2 * ineq_dist @ self.G\n",
    "\n",
    "    def ineq_partial_grad(self, X, Y):\n",
    "        G_effective = self.G[:, self.partial_vars] - self.G[:, self.other_vars] @ (\n",
    "            self._A_other_inv @ self._A_partial\n",
    "        )\n",
    "        h_effective = self.h - (X @ self._A_other_inv.T) @ self.G[:, self.other_vars].T\n",
    "        grad = (\n",
    "            2\n",
    "            * torch.clamp(Y[:, self.partial_vars] @ G_effective.T - h_effective, 0)\n",
    "            @ G_effective\n",
    "        )\n",
    "        Y = torch.zeros(X.shape[0], self.ydim, device=self.device)\n",
    "        Y[:, self.partial_vars] = grad\n",
    "        Y[:, self.other_vars] = -(grad @ self._A_partial.T) @ self._A_other_inv.T\n",
    "        return Y\n",
    "\n",
    "    # Processes intermediate neural network output\n",
    "    def process_output(self, X, Y):\n",
    "        return Y\n",
    "\n",
    "    # Solves for the full set of variables\n",
    "    def complete_partial(self, X, Z):\n",
    "        Y = torch.zeros(X.shape[0], self.ydim, device=self.device)\n",
    "        Y[:, self.partial_vars] = Z\n",
    "        Y[:, self.other_vars] = (X - Z @ self._A_partial.T) @ self._A_other_inv.T\n",
    "        return Y\n",
    "\n",
    "class OriginalQPProblem(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G, b, d, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G, b, d, valid_frac, test_frac)\n",
    "\n",
    "        self._X = self._b\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        self._nineq = self._G.shape[0]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "        self.A_transpose = self._A.T\n",
    "        self.G_transpose = self._G.T\n",
    "\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        # Here, X is the RHS of the equality constraints\n",
    "        return X - Y @ self.A.T\n",
    "\n",
    "    def ineq_resid(self, X, Y):\n",
    "        return Y @ self.G.T - self.d\n",
    "    \n",
    "    def ineq_dist(self, X, Y):\n",
    "        resids = self.ineq_resid(X, Y)\n",
    "        return torch.clamp(resids, 0)\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, A, G, d = self.Q_np, self.p_np, self.A_np, self.G_np, self.d_np\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            for Xi in X_np:\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, G])\n",
    "                my_l = np.hstack([Xi, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([Xi, d])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "                sols = np.array(Y)\n",
    "                parallel_time = total_time / len(X_np)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "\n",
    "class QPProblemVaryingG(SimpleProblem):\n",
    "    def __init__(self, X, Q, p, A, G, b, d, row_indices, col_indices, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G, b, d, valid_frac, test_frac)\n",
    "        # X are the varying values of G\n",
    "        self._X = X\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        self._nineq = self._G.shape[0]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "        self.row_indices = row_indices\n",
    "        self.col_indices = col_indices\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        # Here, X is part of the inequality constraint matrix. So we don't use it\n",
    "        return self.b - Y @ self.A.T\n",
    "\n",
    "    def ineq_resid(self, X, Y):\n",
    "        # Here, X is part of the inequality constraint matrix. So, we need to plug X into the inequality constraint matrix.\n",
    "        G = self.G.expand(X.shape[0], -1, -1).clone()\n",
    "        G[:, self.row_indices, self.col_indices] = X\n",
    "        return torch.bmm(G, Y.unsqueeze(-1)).squeeze(-1) - self.d\n",
    "    \n",
    "    def ineq_dist(self, X, Y):\n",
    "        resids = self.ineq_resid(X, Y)\n",
    "        return torch.clamp(resids, 0)\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        \"\"\"We change op_solve so that the varying G matrices are taken from the input X.\n",
    "        \"\"\"\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, b, d = self.Q_np, self.p_np, self.b_np, self.d_np\n",
    "            G = self.G.expand(X.shape[0], -1, -1).clone()\n",
    "            G[:, self.row_indices, self.col_indices] = X\n",
    "            G = G\n",
    "            A = self.A_np\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "\n",
    "            for Gi in G:\n",
    "                \n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, Gi])\n",
    "                my_l = np.hstack([b, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([b, d])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "            sols = np.array(Y)\n",
    "            parallel_time = total_time / len(G)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "\n",
    "class QPProblemVaryingGbd(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G_base, G_varying, b, d, n_varying_rows, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G_varying, b, d, valid_frac, test_frac)\n",
    "        self.G_base = torch.tensor(G_base)\n",
    "        self.n_varying_rows = n_varying_rows\n",
    "        # Flatten the rows of G that are varying, to be added to the NN input.\n",
    "        G_flattened = self.G[:, :n_varying_rows, :].flatten(start_dim=1)\n",
    "        self._X = torch.concat([G_flattened, self.b, self.d], dim=1)\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        # G now has num_samples in first dimension, num_constraints in second dimension. Take second dimension!\n",
    "        self._nineq = self._G.shape[1]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        \"\"\"B is now varying, we should extract it from X\"\"\"\n",
    "        G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "        return b - Y @ self.A.T\n",
    "\n",
    "    def rebuild_Gbd_from_X(self, X):\n",
    "        # Reshape X to match the first self.n_varying_rows rows of G\n",
    "        G_size = self.n_varying_rows*self.ydim\n",
    "        b_size = self.neq\n",
    "        flattened_G = X[:, :G_size]\n",
    "        b = X[:, G_size:G_size+b_size]\n",
    "        d = X[:, G_size+b_size:]\n",
    "        custom_G = flattened_G.reshape(X.shape[0], self.n_varying_rows, self._ydim)  # Reshape for the batch size\n",
    "\n",
    "        # Take only the first sample of G and clone it for modification\n",
    "        G = self.G_base.clone()  # Shape is (M, P)\n",
    "\n",
    "        # Repeat G for the batch size to avoid memory overlap\n",
    "        G = G.unsqueeze(0).repeat(X.shape[0], 1, 1)  # Shape is (batch_size, M, P)\n",
    "\n",
    "        # Assign custom_G to the first self.n_varying_rows rows of G\n",
    "        G[:, :self.n_varying_rows, :] = custom_G  # Ensure dimensions match\n",
    "        return G, b, d\n",
    "    \n",
    "    def ineq_resid(self, X, Y):\n",
    "        \"\"\"\n",
    "        For the ineq resid, we need to extract the first n rows of the G matrix from it's flattened form X, and plug them into G.\n",
    "        \"\"\"\n",
    "\n",
    "        G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "\n",
    "        # resid = Y @ G.transpose(1, 2) - h\n",
    "        residual = torch.bmm(Y.unsqueeze(1), G.transpose(1, 2)).squeeze(1) - d\n",
    "\n",
    "        # Compute inequality residual\n",
    "        return residual\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        \"\"\"We change op_solve so that the varying G matrices are taken from the input X.\n",
    "        \"\"\"\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, b, d = self.Q_np, self.p_np, self.b_np, self.d_np\n",
    "            G, b, d = self.rebuild_Gbd_from_X(X)\n",
    "            G_np, b_np, d_np = G.detach().cpu().numpy(), b.detach().cpu().numpy(), d.detach().cpu().numpy()\n",
    "            A = self.A_np\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            for idx, Gi in enumerate(G_np):\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, Gi])\n",
    "                my_l = np.hstack([b_np[idx], -np.ones(d_np[idx].shape[0]) * np.inf])\n",
    "                my_u = np.hstack([b_np[idx], d_np[idx]])\n",
    "                solver.setup(\n",
    "                    P=csc_matrix(Q),\n",
    "                    q=p,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "            sols = np.array(Y)\n",
    "            parallel_time = total_time / len(X)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y\n",
    "    \n",
    "\n",
    "class ScaledLPProblem(SimpleProblem):\n",
    "    def __init__(self, Q, p, A, G, b, d, obj_coeff, valid_frac=0.1, test_frac=0.1):\n",
    "        super().__init__(Q, p, A, G, b, d, valid_frac, test_frac)\n",
    "\n",
    "        self._X = self._b\n",
    "        self._c = torch.tensor(obj_coeff)\n",
    "        self._num = self._X.shape[0]\n",
    "        self._neq = self._A.shape[0]\n",
    "        self._nineq = self._G.shape[0]\n",
    "        self._xdim = self._X.shape[1]\n",
    "\n",
    "    def eq_resid(self, X, Y):\n",
    "        return X - Y @ self.A.T\n",
    "\n",
    "    def ineq_resid(self, X, Y):\n",
    "        return Y @ self.G.T - self.d\n",
    "\n",
    "    def obj_fn(self, Y):\n",
    "        return Y @ self._c.T\n",
    "\n",
    "    \n",
    "    def opt_solve(self, X, solver_type=\"osqp\", tol=1e-4):\n",
    "        if solver_type == \"osqp\":\n",
    "            print(\"running osqp\")\n",
    "            Q, p, A, G, d = self.Q_np, self.p_np, self.A_np, self.G_np, self.d_np\n",
    "            c = self._c.numpy()\n",
    "            X_np = X.detach().cpu().numpy()\n",
    "            Y = []\n",
    "            total_time = 0\n",
    "            zero_Q = np.zeros((c.shape[0], c.shape[0]))\n",
    "\n",
    "            for Xi in X_np:\n",
    "                solver = osqp.OSQP()\n",
    "                my_A = np.vstack([A, G])\n",
    "                my_l = np.hstack([Xi, -np.ones(d.shape[0]) * np.inf])\n",
    "                my_u = np.hstack([Xi, d])\n",
    "                solver.setup(\n",
    "                    q=c,\n",
    "                    A=csc_matrix(my_A),\n",
    "                    l=my_l,\n",
    "                    u=my_u,\n",
    "                    verbose=False,\n",
    "                    eps_prim_inf=tol,\n",
    "                )\n",
    "                start_time = time.time()\n",
    "                results = solver.solve()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time += end_time - start_time\n",
    "                if results.info.status == \"solved\":\n",
    "                    Y.append(results.x)\n",
    "                else:\n",
    "                    Y.append(np.ones(self.ydim) * np.nan)\n",
    "\n",
    "                sols = np.array(Y)\n",
    "                parallel_time = total_time / len(X_np)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return sols, total_time, parallel_time\n",
    "\n",
    "    def calc_Y(self):\n",
    "        Y = self.opt_solve(self.X)[0]\n",
    "        feas_mask = ~np.isnan(Y).all(axis=1)\n",
    "        self._num = feas_mask.sum()\n",
    "        self._X = self._X[feas_mask]\n",
    "        self._Y = torch.tensor(Y[feas_mask])\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_QP_dataset(num_var, num_ineq, num_eq, num_examples):\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/QP_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, vary):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    b = np.random.uniform(-1, 1, size=(num_eq))\n",
    "    G = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    d = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    X = np.random.normal(loc=0, scale=1., size=(num_examples, num_ineq))\n",
    "\n",
    "    # Try first with changing a single row!\n",
    "    if vary == 'row':\n",
    "        row_indices = [0] * num_ineq\n",
    "        col_indices = list(range(num_ineq))\n",
    "    if vary == 'column':\n",
    "        col_indices = [0] * num_ineq\n",
    "        row_indices = list(range(num_ineq))\n",
    "    if vary == 'random':\n",
    "        col_indices = np.random.choice(num_var, num_ineq, replace=False)\n",
    "        row_indices = np.random.choice(num_ineq, num_ineq, replace=True)\n",
    "\n",
    "    problem = QPProblemVaryingG(X=torch.tensor(X), Q=Q, p=p, A=A, G=G, b=b, d=d, row_indices=row_indices, col_indices=col_indices)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/Varying_G_type={}_dataset_var{}_ineq{}_eq{}_ex{}\".format(vary, num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_b_d_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    B = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(num_varying_rows, num_var))\n",
    "        G_list.append(G_sample)\n",
    "    \n",
    "    # Create H matrix for each example\n",
    "    D_list = []\n",
    "    for Gi in G_list:\n",
    "        d = np.sum(np.abs(Gi @ np.linalg.pinv(A)), axis=1)  # Compute bounds for all inequalities\n",
    "        D_list.append(d)  # Resulting shape will be (num_ineq,)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    D = np.stack(D_list, axis=0)  # Shape (num_examples, num_ineq)\n",
    "    problem = QPProblemVaryingGbd(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=B, d=D, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_scaled_QP_problem(num_var, num_ineq, num_eq, num_examples, scale='normal'):\n",
    "    if scale == 'normal':\n",
    "        obj_scale = 1\n",
    "        var_scale = 1\n",
    "        rhs_scale = 1\n",
    "    elif scale == 'large':\n",
    "        obj_scale = 1e9\n",
    "        var_scale = 1e3\n",
    "        rhs_scale = 1e3\n",
    "\n",
    "    np.random.seed(17)\n",
    "\n",
    "    Q = np.diag(np.random.random(num_var)) * obj_scale\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=var_scale, size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-rhs_scale, rhs_scale, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=var_scale, size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    # with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        # pickle.dump(problem, f)\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "DTYPE = torch.float64\n",
    "DEVICE = torch.device=\"cpu\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(42)\n",
    "print(f\"Running on {DEVICE}\")\n",
    "        \n",
    "class PrimalDualTrainer():\n",
    "\n",
    "    def __init__(self, data, args, save_dir):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            data (_type_): _description_\n",
    "            args (_type_): _description_\n",
    "            save_dir (_type_): _description_\n",
    "            problem_type (str, optional): Either \"GEP\" or \"Benchmark\". Defaults to \"GEP\".\n",
    "            log (bool, optional): _description_. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"X dim: {data.xdim}\")\n",
    "        print(f\"Y dim: {data.ydim}\")\n",
    "\n",
    "        print(f\"Size of mu: {data.nineq}\")\n",
    "        print(f\"Size of lambda: {data.neq}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.outer_iterations = args[\"outer_iterations\"]\n",
    "        self.inner_iterations = args[\"inner_iterations\"]\n",
    "        self.tau = args[\"tau\"]\n",
    "        self.rho = args[\"rho\"]\n",
    "        self.rho_max = args[\"rho_max\"]\n",
    "        self.alpha = args[\"alpha\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.hidden_sizes = args[\"hidden_sizes\"]\n",
    "\n",
    "        self.primal_lr = args[\"primal_lr\"]\n",
    "        self.dual_lr = args[\"dual_lr\"]\n",
    "        self.decay = args[\"decay\"]\n",
    "        self.patience = args[\"patience\"]\n",
    "        \n",
    "        # for logging\n",
    "        self.step = 0\n",
    "\n",
    "        X = data.X\n",
    "\n",
    "        train = data.train_indices\n",
    "        valid = data.valid_indices\n",
    "        test = data.test_indices\n",
    "\n",
    "        # Traning data in a data set\n",
    "        #! Vary per experiment\n",
    "        self.train_dataset = TensorDataset(X[train].to(DEVICE))\n",
    "        self.valid_dataset = TensorDataset(X[valid].to(DEVICE))\n",
    "        self.test_dataset = TensorDataset(X[test].to(DEVICE))\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=1000, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        self.primal_net = PrimalNet(self.data, self.hidden_sizes).to(dtype=DTYPE, device=DEVICE)\n",
    "        self.dual_net = DualNet(self.data, self.hidden_sizes, self.data.nineq, self.data.neq).to(dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "        self.primal_optim = torch.optim.Adam(self.primal_net.parameters(), lr=self.primal_lr)\n",
    "        self.dual_optim = torch.optim.Adam(self.dual_net.parameters(), lr=self.dual_lr)\n",
    "\n",
    "        # Add schedulers\n",
    "        self.primal_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.primal_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "        self.dual_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.dual_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "\n",
    "    def train_PDL(self, max_violation_save_thresholds=[0.005, 0.006, 0.007, 0.008, 0.009, 0.01]):\n",
    "        try:\n",
    "            best_val_losses = [0] * len(max_violation_save_thresholds)\n",
    "            prev_v_k = 0\n",
    "            training_time = 0\n",
    "            stats = {}\n",
    "            stats[\"training_time\"] = {}\n",
    "            for k in range(self.outer_iterations):\n",
    "                begin_time = time.time()\n",
    "                frozen_dual_net = copy.deepcopy(self.dual_net)\n",
    "                # self.logger.log_rho_vk(self.rho, prev_v_k, self.step)\n",
    "                for l1 in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update primal net using primal loss\n",
    "                    self.primal_net.train()\n",
    "\n",
    "                    # Accumulate training loss over all batches\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        Xtrain = Xtrain[0]\n",
    "                        batch_start = time.time()\n",
    "                        self.primal_optim.zero_grad()\n",
    "                        y = self.primal_net(Xtrain)\n",
    "                        with torch.no_grad():\n",
    "                            mu, lamb = frozen_dual_net(Xtrain)\n",
    "                        batch_loss = self.primal_loss(Xtrain, y, mu, lamb).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.primal_optim.step()\n",
    "                        training_time += time.time() - batch_start\n",
    "\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    with torch.no_grad():\n",
    "                        self.primal_net.eval()\n",
    "                        frozen_dual_net.eval()\n",
    "                        obj_val_mean, val_loss_mean, ineq_max, ineq_mean, eq_max, eq_mean = self.evaluate(self.valid_dataset.tensors[0], self.primal_net, self.dual_net)    \n",
    "                        \n",
    "                        # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                        self.primal_scheduler.step(torch.sign(val_loss_mean) * (torch.abs(val_loss_mean) / self.rho))\n",
    "\n",
    "                        # Save if best model:\n",
    "                        for i in range(len(max_violation_save_thresholds)):\n",
    "                            if ineq_max < max_violation_save_thresholds[i] \\\n",
    "                            and eq_max < max_violation_save_thresholds[i] \\\n",
    "                            and obj_val_mean < best_val_losses[i]:\n",
    "                                print(f\"Saving new model with obj: {obj_val_mean}, eq_max: {eq_max}, ineq_max: {ineq_max}, eq_mean: {eq_mean}, ineq_mean: {ineq_mean}\")\n",
    "                                with open(os.path.join(self.save_dir, f'{max_violation_save_thresholds[i]}_primal_net.dict'), 'wb') as f:\n",
    "                                    torch.save(self.primal_net.state_dict(), f)\n",
    "                                with open(os.path.join(self.save_dir, f'{max_violation_save_thresholds[i]}_dual_net.dict'), 'wb') as f:\n",
    "                                    torch.save(self.dual_net.state_dict(), f)\n",
    "                                best_val_losses[i] = obj_val_mean\n",
    "\n",
    "                                stats[\"training_time\"][f\"{max_violation_save_thresholds[i]}\"] = training_time\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Copy primal net into frozen primal net\n",
    "                    frozen_primal_net = copy.deepcopy(self.primal_net)\n",
    "                    X = self.train_dataset.tensors[0]\n",
    "                    # Calculate v_k\n",
    "                    y = frozen_primal_net(X)\n",
    "                    mu_k, lamb_k = frozen_dual_net(X)\n",
    "                    v_k = self.violation(X, y, mu_k)\n",
    "\n",
    "                for l in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update dual net using dual loss\n",
    "                    self.dual_net.train()\n",
    "                    frozen_primal_net.train()\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        Xtrain = Xtrain[0]\n",
    "                        batch_start = time.time()\n",
    "                        self.dual_optim.zero_grad()\n",
    "                        mu, lamb = self.dual_net(Xtrain)\n",
    "                        with torch.no_grad():\n",
    "                            mu_k, lamb_k = frozen_dual_net(Xtrain)\n",
    "                            y = frozen_primal_net(Xtrain)\n",
    "                        batch_loss = self.dual_loss(Xtrain, y, mu, lamb, mu_k, lamb_k).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.dual_optim.step()\n",
    "                        training_time += time.time() - batch_start\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    with torch.no_grad():\n",
    "                        frozen_primal_net.eval()\n",
    "                        self.dual_net.eval()\n",
    "                        obj_val_mean, val_loss_mean, ineq_max, ineq_mean, eq_max, eq_mean = self.evaluate(self.valid_dataset.tensors[0], self.primal_net, self.dual_net)    \n",
    "                        # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                        self.dual_scheduler.step(torch.sign(val_loss_mean) * (torch.abs(val_loss_mean) / self.rho))\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(\"-\"*40)\n",
    "                print(f\"Epoch {k} done. Time taken: {end_time - begin_time}. Rho: {self.rho}. Primal LR: {self.primal_optim.param_groups[0]['lr']}, Dual LR: {self.dual_optim.param_groups[0]['lr']}\")\n",
    "\n",
    "                # Update rho from the second iteration onward.\n",
    "                if k > 0 and v_k > self.tau * prev_v_k:\n",
    "                    self.rho = np.min([self.alpha * self.rho, self.rho_max])\n",
    "\n",
    "                prev_v_k = v_k\n",
    "            \n",
    "                print(f\"Validation set evaluate:\")\n",
    "                with torch.no_grad():\n",
    "                    self.primal_net.eval()\n",
    "                    self.dual_net.eval()\n",
    "                    obj_val_mean, val_loss_mean, ineq_max, ineq_mean, eq_max, eq_mean = self.evaluate(self.valid_dataset.tensors[0], self.primal_net, self.dual_net)\n",
    "                    print(f\"obj_val_mean: {obj_val_mean}, val_loss_mean: {val_loss_mean}, ineq_max: {ineq_max}, ineq_mean: {ineq_mean}, eq_max: {eq_max}, eq_mean: {eq_mean}\")    \n",
    "                    \n",
    "            print(\"-\"*40)\n",
    "            print(f\"Test set evaluate:\")\n",
    "            with torch.no_grad():\n",
    "                self.primal_net.eval()\n",
    "                self.dual_net.eval()\n",
    "                obj_val_mean, test_loss_mean, ineq_max, ineq_mean, eq_max, eq_mean = self.evaluate(self.test_dataset.tensors[0], self.primal_net, self.dual_net)\n",
    "                print(f\"obj_val_mean: {obj_val_mean}, val_loss_mean: {test_loss_mean}, ineq_max: {ineq_max}, ineq_mean: {ineq_mean}, eq_max: {eq_max}, eq_mean: {eq_mean}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e, flush=True)\n",
    "            # # Ensure writer is closed even if an exception occurs\n",
    "            # if self.logger:\n",
    "            #     self.logger.close()\n",
    "            raise\n",
    "\n",
    "        with open(os.path.join(self.save_dir, 'stats.dict'), 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "        with open(os.path.join(self.save_dir, 'primal_net.dict'), 'wb') as f:\n",
    "            torch.save(self.primal_net.state_dict(), f)\n",
    "        with open(os.path.join(self.save_dir, 'dual_net.dict'), 'wb') as f:\n",
    "            torch.save(self.dual_net.state_dict(), f)\n",
    "\n",
    "        return self.primal_net, self.dual_net, stats\n",
    "\n",
    "    def evaluate(self, X, primal_net, dual_net):        \n",
    "        # Forward pass through networks\n",
    "        Y = primal_net(X)\n",
    "        mu, lamb = dual_net(X)\n",
    "\n",
    "        ineq_dist = self.data.ineq_dist(X, Y)\n",
    "        eq_resid = self.data.eq_resid(X, Y)\n",
    "\n",
    "        # Convert lists to arrays for easier handling\n",
    "        obj_values = self.data.obj_fn(Y).detach()\n",
    "        primal_losses = self.primal_loss(X, Y, mu, lamb).detach()\n",
    "        ineq_max_vals = torch.max(ineq_dist, dim=1)[0].detach()\n",
    "        ineq_mean_vals = torch.mean(ineq_dist, dim=1).detach()\n",
    "        eq_max_vals = torch.max(torch.abs(eq_resid), dim=1)[0].detach()\n",
    "        eq_mean_vals = torch.mean(torch.abs(eq_resid), dim=1).detach()\n",
    "\n",
    "        return torch.mean(obj_values), torch.mean(primal_losses), torch.mean(ineq_max_vals), torch.mean(ineq_mean_vals), torch.mean(eq_max_vals), torch.mean(eq_mean_vals)\n",
    "\n",
    "\n",
    "\n",
    "    def primal_loss(self, x, y, mu, lamb):\n",
    "        obj = self.data.obj_fn(y)\n",
    "        \n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(x, y)\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(x, y)\n",
    "\n",
    "        # ! Clamp mu?\n",
    "        # Element-wise clamping of mu_i when g_i (ineq) is negative\n",
    "        # mu = torch.where(ineq < 0, torch.zeros_like(mu), mu)\n",
    "        # ! Clamp ineq_resid?\n",
    "        # ineq = ineq.clamp(min=0)\n",
    "\n",
    "        lagrange_ineq = torch.sum(mu * ineq, dim=1)  # Shape (batch_size,)\n",
    "\n",
    "        lagrange_eq = torch.sum(lamb * eq, dim=1)   # Shape (batch_size,)\n",
    "\n",
    "        violation_ineq = torch.sum(torch.maximum(ineq, torch.zeros_like(ineq)) ** 2, dim=1)\n",
    "        violation_eq = torch.sum(eq ** 2, dim=1)\n",
    "        penalty = self.rho/2 * (violation_ineq + violation_eq)\n",
    "\n",
    "        loss = (obj + (lagrange_ineq + lagrange_eq + penalty))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def dual_loss(self, x, y, mu, lamb, mu_k, lamb_k):\n",
    "        # mu = [batch, g]\n",
    "        # lamb = [batch, h]\n",
    "\n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(x, y) # [batch, g]\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(x, y)   # [batch, h]\n",
    "\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        target_mu = torch.maximum(mu_k + self.rho * ineq, torch.zeros_like(ineq))\n",
    "        # target_mu = torch.maximum(mu_k + 1e-1 * ineq, torch.zeros_like(ineq))\n",
    "\n",
    "        dual_resid_ineq = mu - target_mu # [batch, g]\n",
    "\n",
    "        dual_resid_ineq = torch.norm(dual_resid_ineq, dim=1)  # [batch]\n",
    "\n",
    "        # Compute the dual residuals for equality constraints\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        dual_resid_eq = lamb - (lamb_k + self.rho * eq)\n",
    "        # dual_resid_eq = lamb - (lamb_k + 1e-1 * eq)\n",
    "        dual_resid_eq = torch.norm(dual_resid_eq, dim=1)  # Norm along constraint dimension\n",
    "\n",
    "        loss = (dual_resid_ineq + dual_resid_eq)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def violation(self, x, y, mu_k):\n",
    "        # Calculate the equality constraint function h_x(y)\n",
    "        eq = self.data.eq_resid(x, y)  # Assume shape (num_samples, n_eq)\n",
    "        \n",
    "        # Calculate the infinity norm of h_x(y)\n",
    "        eq_inf_norm = torch.abs(eq).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Calculate the inequality constraint function g_x(y)\n",
    "        ineq = self.data.ineq_resid(x, y)  # Assume shape (num_samples, n_ineq)\n",
    "        \n",
    "        # Calculate sigma_x(y) for each inequality constraint\n",
    "        sigma_y = torch.maximum(ineq, -mu_k / self.rho)  # Element-wise max\n",
    "        \n",
    "        # Calculate the infinity norm of sigma_x(y)\n",
    "        sigma_y_inf_norm = torch.abs(sigma_y).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Compute v_k as the maximum of the two norms\n",
    "        v_k = torch.maximum(eq_inf_norm, sigma_y_inf_norm)  # Shape: (num_samples,)\n",
    "        \n",
    "        return v_k.max().item()\n",
    "\n",
    "class PrimalNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        \n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes + [data.ydim]\n",
    "        layers = []\n",
    "\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            if out_size != data.ydim:  # Add ReLU activation for hidden layers only\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DualNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes, mu_size, lamb_size):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        self._mu_size = mu_size\n",
    "        self._lamb_size = lamb_size\n",
    "\n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes\n",
    "        # layer_sizes = [2*data.xdim + 1000] + self._hidden_sizes\n",
    "        layers = []\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        # Add the output layer\n",
    "        self.out_layer = nn.Linear(self._hidden_sizes[-1], self._mu_size + self._lamb_size)\n",
    "        nn.init.zeros_(self.out_layer.weight)  # Initialize output layer weights to 0\n",
    "        nn.init.zeros_(self.out_layer.bias)    # Initialize output layer biases to 0\n",
    "        layers.append(self.out_layer)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out_mu = out[:, :self._mu_size]\n",
    "        out_lamb = out[:, self._mu_size:]\n",
    "        return out_mu, out_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var = 100\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 10000\n",
    "\n",
    "args = {\n",
    "    \"outer_iterations\": 10,\n",
    "    \"inner_iterations\": 500,\n",
    "    \"tau\": 0.8,\n",
    "    \"rho\": 0.5,\n",
    "    \"rho_max\": 5000,\n",
    "    \"alpha\": 10,\n",
    "    \"batch_size\": 100,\n",
    "    \"hidden_sizes\": [500, 500],\n",
    "    \"primal_lr\": 1e-4,\n",
    "    \"dual_lr\": 1e-4,\n",
    "    \"decay\": 0.99,\n",
    "    \"patience\": 10,\n",
    "    \"corrEps\": 1e-4,\n",
    "    \"train\": 0.001,\n",
    "    \"valid\": 0.001,\n",
    "    \"test\": 0.998\n",
    "}\n",
    "\n",
    "save_dir = \"benchmark_experiment_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "10000\n",
      "running osqp\n",
      "10000\n",
      "running osqp\n",
      "10000\n",
      "running osqp\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "original_data = create_QP_dataset(num_var, num_ineq, num_eq, num_examples)\n",
    "varying_cm_row_data = create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, vary='row')\n",
    "varying_cm_column_data = create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, vary='column')\n",
    "varying_cm_random_data = create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, vary='random')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n",
      "----------------------------------------\n",
      "Epoch 0 done. Time taken: 795.4284920692444. Rho: 0.5. Primal LR: 0.0001, Dual LR: 6.361854860638712e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -17.029636299487052, val_loss_mean: -14.783663967877542, ineq_max: 0.4449866324832299, ineq_mean: 0.03227994727998439, eq_max: 0.6400913768819525, eq_mean: 0.1689336782741651\n",
      "Saving new model with obj: -15.553748111246234, eq_max: 0.059174828248396566, ineq_max: 0.1437334060849282, eq_mean: 0.015171898560180523, ineq_mean: 0.004033011678070988\n",
      "Saving new model with obj: -15.553748111246234, eq_max: 0.059174828248396566, ineq_max: 0.1437334060849282, eq_mean: 0.015171898560180523, ineq_mean: 0.004033011678070988\n",
      "Saving new model with obj: -15.568131077446363, eq_max: 0.058380374627659534, ineq_max: 0.1405465546355575, eq_mean: 0.014992108876071832, ineq_mean: 0.003920567010817629\n",
      "Saving new model with obj: -15.568131077446363, eq_max: 0.058380374627659534, ineq_max: 0.1405465546355575, eq_mean: 0.014992108876071832, ineq_mean: 0.003920567010817629\n",
      "----------------------------------------\n",
      "Epoch 1 done. Time taken: 772.446674823761. Rho: 0.5. Primal LR: 6.361854860638712e-05, Dual LR: 4.430479816261728e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.557398062718724, val_loss_mean: -15.137633320932306, ineq_max: 0.11088630607645865, ineq_mean: 0.0036795295017935563, eq_max: 0.04685095163571359, eq_mean: 0.011016476098310291\n",
      "----------------------------------------\n",
      "Epoch 2 done. Time taken: 892.7826459407806. Rho: 5.0. Primal LR: 4.04731972678324e-05, Dual LR: 2.8186069554046354e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.562723054811999, val_loss_mean: -14.950579755025247, ineq_max: 0.16763601881245982, ineq_mean: 0.0047040640078709044, eq_max: 0.010751858163786687, eq_mean: 0.0034654546071734476\n",
      "----------------------------------------\n",
      "Epoch 3 done. Time taken: 852.9910049438477. Rho: 5.0. Primal LR: 2.549097606963093e-05, Dual LR: 1.7752252675876342e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.497842084391621, val_loss_mean: -10.09791770992868, ineq_max: 0.14716498377557408, ineq_mean: 0.003242630880046293, eq_max: 0.010151694622610766, eq_mean: 0.0033920969009330368\n",
      "----------------------------------------\n",
      "Epoch 4 done. Time taken: 5985.632386922836. Rho: 50.0. Primal LR: 1.6216989001100652e-05, Dual LR: 1.1293725497331048e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.524447339317975, val_loss_mean: -10.391849717291356, ineq_max: 0.14572289733364605, ineq_mean: 0.003773905780397145, eq_max: 0.009061236177356656, eq_mean: 0.0029921202729269814\n",
      "----------------------------------------\n",
      "Epoch 5 done. Time taken: 834.8626518249512. Rho: 50.0. Primal LR: 1.0213842899856094e-05, Dual LR: 7.113055202541569e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.525354509900072, val_loss_mean: -10.208008903146704, ineq_max: 0.14629936155011528, ineq_mean: 0.003808729156018571, eq_max: 0.008314331769877703, eq_mean: 0.002738919662141854\n",
      "----------------------------------------\n",
      "Epoch 6 done. Time taken: 753.2278821468353. Rho: 50.0. Primal LR: 6.497898609824964e-06, Dual LR: 4.525222481428054e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.529445548012234, val_loss_mean: 42.73682420398227, ineq_max: 0.1613366311258189, ineq_mean: 0.004088478386376703, eq_max: 0.007978497189400776, eq_mean: 0.002626669264266582\n",
      "----------------------------------------\n",
      "Epoch 7 done. Time taken: 831.2553308010101. Rho: 500.0. Primal LR: 4.0925300976303945e-06, Dual LR: 2.8500920552555178e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.500557120271285, val_loss_mean: 38.79150335811061, ineq_max: 0.1424116747355378, ineq_mean: 0.0035024668708818333, eq_max: 0.00774715052388976, eq_mean: 0.0025495131348626495\n",
      "----------------------------------------\n",
      "Epoch 8 done. Time taken: 932.401554107666. Rho: 500.0. Primal LR: 2.6036082493920133e-06, Dual LR: 1.8131871994995088e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.500968410290135, val_loss_mean: 567.7661279378757, ineq_max: 0.16145916882497813, ineq_mean: 0.004030837253768942, eq_max: 0.007720150504487307, eq_mean: 0.0025402177273237594\n",
      "----------------------------------------\n",
      "Epoch 9 done. Time taken: 893.6889116764069. Rho: 5000.0. Primal LR: 1.6398140018627685e-06, Dual LR: 1.1419881460399999e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -15.55410602495003, val_loss_mean: 536.1063484494761, ineq_max: 0.1453044391552951, ineq_mean: 0.0036369851102771044, eq_max: 0.007710849798170937, eq_mean: 0.0025329204780790396\n",
      "----------------------------------------\n",
      "Test set evaluate:\n",
      "obj_val_mean: -15.550564071559547, val_loss_mean: 563.0223694229168, ineq_max: 0.14930233044897034, ineq_mean: 0.0037679928636382954, eq_max: 0.008793032417716116, eq_mean: 0.0027671527938738167\n",
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n",
      "----------------------------------------\n",
      "Epoch 0 done. Time taken: 934.2817161083221. Rho: 0.5. Primal LR: 0.0001, Dual LR: 6.361854860638712e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -16.45539966844867, val_loss_mean: -13.770082843136333, ineq_max: 0.7537982772959119, ineq_mean: 0.033369229732382676, eq_max: 0.6148532586487617, eq_mean: 0.17786818070552904\n",
      "----------------------------------------\n",
      "Epoch 1 done. Time taken: 912.6159009933472. Rho: 0.5. Primal LR: 6.361854860638712e-05, Dual LR: 4.04731972678324e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -10.634631893833316, val_loss_mean: -4.325036276467095, ineq_max: 0.5392142059095905, ineq_mean: 0.020759267088592652, eq_max: 0.11519827404785471, eq_mean: 0.0417793656011001\n",
      "----------------------------------------\n",
      "Epoch 2 done. Time taken: 837.0679821968079. Rho: 5.0. Primal LR: 4.04731972678324e-05, Dual LR: 2.549097606963093e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.172558637657968, val_loss_mean: -13.191815303659714, ineq_max: 0.35799535437873536, ineq_mean: 0.009528820908914187, eq_max: 0.015992213354300736, eq_mean: 0.005051206533340872\n",
      "----------------------------------------\n",
      "Epoch 3 done. Time taken: 845.4728920459747. Rho: 5.0. Primal LR: 2.549097606963093e-05, Dual LR: 1.6216989001100652e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.181468305413079, val_loss_mean: -9.619787930965689, ineq_max: 0.18040041668658488, ineq_mean: 0.00524998399664109, eq_max: 0.016549568895113147, eq_mean: 0.0048641109784389605\n",
      "----------------------------------------\n",
      "Epoch 4 done. Time taken: 916.7367157936096. Rho: 50.0. Primal LR: 1.6216989001100652e-05, Dual LR: 1.0213842899856094e-05\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.384531790185218, val_loss_mean: -7.5558641923027645, ineq_max: 0.27062642606699056, ineq_mean: 0.007952717097573584, eq_max: 0.00514241240807067, eq_mean: 0.0017232844721535518\n",
      "----------------------------------------\n",
      "Epoch 5 done. Time taken: 900.0071060657501. Rho: 50.0. Primal LR: 1.0213842899856094e-05, Dual LR: 6.497898609824964e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.270783040409402, val_loss_mean: -9.760026929043248, ineq_max: 0.1748868504997452, ineq_mean: 0.00543610816987935, eq_max: 0.004370223168380534, eq_mean: 0.0014939984240549891\n",
      "----------------------------------------\n",
      "Epoch 6 done. Time taken: 830.0051980018616. Rho: 50.0. Primal LR: 6.497898609824964e-06, Dual LR: 4.0925300976303945e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -13.317958781940504, val_loss_mean: 39.00063956412755, ineq_max: 0.19670871125137807, ineq_mean: 0.006038814727620379, eq_max: 0.004147277177375575, eq_mean: 0.0014155399173131737\n",
      "----------------------------------------\n",
      "Epoch 7 done. Time taken: 848.3451521396637. Rho: 500.0. Primal LR: 4.0925300976303945e-06, Dual LR: 2.6036082493920133e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.352158567967832, val_loss_mean: 60.740337165994674, ineq_max: 0.2984881045079176, ineq_mean: 0.009147363766293217, eq_max: 0.0025411643046827502, eq_mean: 0.0008486509723759882\n",
      "----------------------------------------\n",
      "Epoch 8 done. Time taken: 59413.99885702133. Rho: 500.0. Primal LR: 2.6036082493920133e-06, Dual LR: 1.6398140018627685e-06\n",
      "Validation set evaluate:\n",
      "obj_val_mean: -14.384532573371768, val_loss_mean: 855.5731883444736, ineq_max: 0.32749015820381683, ineq_mean: 0.010578025218382522, eq_max: 0.0025963646760768745, eq_mean: 0.0008401119020660228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m row_primal_net, row_dual_net, _ \u001b[38;5;241m=\u001b[39m row_trainer\u001b[38;5;241m.\u001b[39mtrain_PDL(max_violation_save_thresholds)\n\u001b[1;32m      9\u001b[0m column_trainer \u001b[38;5;241m=\u001b[39m PrimalDualTrainer(varying_cm_column_data, args, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m col_primal_net, col_dual_net, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_PDL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m random_trainer \u001b[38;5;241m=\u001b[39m PrimalDualTrainer(varying_cm_random_data, args, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m random_primal_net, random_dual_net, _ \u001b[38;5;241m=\u001b[39m random_trainer\u001b[38;5;241m.\u001b[39mtrain_PDL()\n",
      "Cell \u001b[0;32mIn[178], line 98\u001b[0m, in \u001b[0;36mPrimalDualTrainer.train_PDL\u001b[0;34m(self, max_violation_save_thresholds)\u001b[0m\n\u001b[1;32m     96\u001b[0m batch_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimal_optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 98\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimal_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    100\u001b[0m     mu, lamb \u001b[38;5;241m=\u001b[39m frozen_dual_net(Xtrain)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[178], line 324\u001b[0m, in \u001b[0;36mPrimalNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/site-packages/torch/fx/traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/miniconda3/envs/thesis/lib/python3.9/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# original_trainer = PrimalDualTrainer(original_data, args, os.path.join(save_dir, 'original'))\n",
    "# original_primal_net, original_dual_net, _ = original_trainer.train_PDL()\n",
    "\n",
    "max_violation_save_thresholds = [0.07, 0.08, 0.09, 0.1, 0.15, 0.2]\n",
    "\n",
    "# row_trainer = PrimalDualTrainer(varying_cm_row_data, args, os.path.join(save_dir, 'row'))\n",
    "# row_primal_net, row_dual_net, _ = row_trainer.train_PDL(max_violation_save_thresholds)\n",
    "\n",
    "column_trainer = PrimalDualTrainer(varying_cm_column_data, args, os.path.join(save_dir, 'column'))\n",
    "col_primal_net, col_dual_net, _ = column_trainer.train_PDL(max_violation_save_thresholds)\n",
    "\n",
    "random_trainer = PrimalDualTrainer(varying_cm_random_data, args, os.path.join(save_dir, 'random'))\n",
    "random_primal_net, random_dual_net, _ = random_trainer.train_PDL(max_violation_save_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.03574072137141 -14.159263032697682 0.05836505624044085 0.00017314761295659365 4.273874887204891e-06 0.0013226164497767041 0.0004215035947090046\n",
      "-15.571241360476339 -15.346537498605803 0.014434519866945456 0.029326313612115122 0.0008174214563382847 0.0019247002016841645 0.0005937570470690621\n",
      "-15.353276156344776 -14.371511671264603 0.06354776121051121 0.02001070985463688 0.0004715649132288072 0.0008491038598132157 0.00028472452067880653\n",
      "-15.609351985154028 -14.481196481463954 0.07228137394973655 0.05245784645211027 0.0012259314204511058 0.0015708231500230776 0.0004784480383306505\n"
     ]
    }
   ],
   "source": [
    "for model, data in [(original_primal_net, original_data), (row_primal_net, varying_cm_row_data), (col_primal_net, varying_cm_column_data), (random_primal_net, varying_cm_random_data)]:\n",
    "    Y_pred = model(data.X)\n",
    "    obj_known = data.obj_fn(data.Y).detach().cpu().numpy()\n",
    "    obj_pred = data.obj_fn(Y_pred).detach().cpu().numpy()\n",
    "    obj_gap = ((obj_known - obj_pred)/obj_known).mean()\n",
    "\n",
    "    ineq_dist = data.ineq_dist(data.X, Y_pred)\n",
    "    eq_resid = data.eq_resid(data.X, Y_pred)\n",
    "\n",
    "    ineq_max_vals = torch.max(ineq_dist, dim=1)[0].detach().cpu().numpy().mean()\n",
    "    ineq_mean_vals = torch.mean(ineq_dist, dim=1).detach().cpu().numpy().mean()\n",
    "    eq_max_vals = torch.max(torch.abs(eq_resid), dim=1)[0].detach().cpu().numpy().mean()\n",
    "    eq_mean_vals = torch.mean(torch.abs(eq_resid), dim=1).detach().cpu().numpy().mean()\n",
    "    \n",
    "    print(obj_known.mean(), obj_pred.mean(), obj_gap, ineq_max_vals, ineq_mean_vals, eq_max_vals, eq_mean_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': {}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"benchmark_experiment_output/original/stats.dict\", \"rb\") as f:\n",
    "    stats = pickle.load(f)\n",
    "    print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

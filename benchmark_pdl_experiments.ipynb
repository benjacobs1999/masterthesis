{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.autograd.profiler as profiler\n",
    "import numpy as np\n",
    "\n",
    "from QP_problem import SimpleProblem, OriginalQPProblem, QPProblemVaryingG, QPProblemVaryingGbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_QP_dataset(num_var, num_ineq, num_eq, num_examples):\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    b = np.random.uniform(-1, 1, size=(num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "    # TODO: Can we keep h constant, if we are varying G?\n",
    "    d = np.sum(np.abs(G_base@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(1, num_var))\n",
    "        G_list.append(G_sample)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    problem = QPProblemVaryingG(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=b, d=d, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_varying_G_b_d_dataset(num_var, num_ineq, num_eq, num_examples, num_varying_rows):\n",
    "    \"\"\"Creates a modified QP data set that differs in the inequality constraint matrix, instead of the RHS variables.\n",
    "    \"\"\"\n",
    "    np.random.seed(17)\n",
    "    Q = np.diag(np.random.random(num_var))\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=1., size=(num_eq, num_var))\n",
    "    # X is the same for all samples:\n",
    "    B = np.random.uniform(-1, 1, size=(num_examples, num_eq))\n",
    "    G_base = np.random.normal(loc=0, scale=1., size=(num_ineq, num_var))\n",
    "\n",
    "    G_list = []\n",
    "    # For each sample, create a different inequality constraint matrix\n",
    "    for _ in range(num_examples):\n",
    "        G_sample = G_base.copy()\n",
    "        # Vary the first n rows, (specified by num_varying_rows).\n",
    "        G_sample[:num_varying_rows, :] = np.random.normal(loc=0, scale=1., size=(num_varying_rows, num_var))\n",
    "        G_list.append(G_sample)\n",
    "    \n",
    "    # Create H matrix for each example\n",
    "    D_list = []\n",
    "    for Gi in G_list:\n",
    "        d = np.sum(np.abs(Gi @ np.linalg.pinv(A)), axis=1)  # Compute bounds for all inequalities\n",
    "        D_list.append(d)  # Resulting shape will be (num_ineq,)\n",
    "\n",
    "    G = np.array(G_list)\n",
    "    D = np.stack(D_list, axis=0)  # Shape (num_examples, num_ineq)\n",
    "    problem = QPProblemVaryingGbd(Q=Q, p=p, A=A, G_base=G_base, G_varying=G, b=B, d=D, n_varying_rows=num_varying_rows)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    with open(\"./QP_data/modified/MODIFIED_random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        pickle.dump(problem, f)\n",
    "    \n",
    "    return problem\n",
    "\n",
    "def create_scaled_QP_problem(num_var, num_ineq, num_eq, num_examples, scale='normal'):\n",
    "    if scale == 'normal':\n",
    "        obj_scale = 1\n",
    "        var_scale = 1\n",
    "        rhs_scale = 1\n",
    "    elif scale == 'large':\n",
    "        obj_scale = 1e9\n",
    "        var_scale = 1e3\n",
    "        rhs_scale = 1e3\n",
    "\n",
    "    np.random.seed(17)\n",
    "\n",
    "    Q = np.diag(np.random.random(num_var)) * obj_scale\n",
    "    p = np.random.random(num_var)\n",
    "    A = np.random.normal(loc=0, scale=var_scale, size=(num_eq, num_var))\n",
    "    X = np.random.uniform(-rhs_scale, rhs_scale, size=(num_examples, num_eq))\n",
    "    G = np.random.normal(loc=0, scale=var_scale, size=(num_ineq, num_var))\n",
    "    h = np.sum(np.abs(G@np.linalg.pinv(A)), axis=1)\n",
    "\n",
    "    problem = OriginalQPProblem(Q, p, A, G, X, h)\n",
    "    problem.calc_Y()\n",
    "    print(len(problem.Y))\n",
    "\n",
    "    # with open(\"./QP_data/original/random_simple_dataset_var{}_ineq{}_eq{}_ex{}\".format(num_var, num_ineq, num_eq, num_examples), 'wb') as f:\n",
    "        # pickle.dump(problem, f)\n",
    "    \n",
    "    return problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "DTYPE = torch.float64\n",
    "DEVICE = torch.device=\"cpu\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(42)\n",
    "print(f\"Running on {DEVICE}\")\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, eq_cm, ineq_cm, eq_rhs, ineq_rhs):\n",
    "        self.x = x\n",
    "        self.eq_cm = eq_cm \n",
    "        self.ineq_cm = ineq_cm\n",
    "        self.eq_rhs = eq_rhs\n",
    "        self.ineq_rhs = ineq_rhs\n",
    "        self._index = 0  # Internal index for tracking iteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a tuple of input and target for the given index\n",
    "        #! Change per data set.\n",
    "        # return self.x[idx], self.eq_cm[idx], self.ineq_cm[idx], self.eq_rhs[idx], self.ineq_rhs[idx]\n",
    "        return self.x[idx]\n",
    "\n",
    "        \n",
    "class PrimalDualTrainer():\n",
    "\n",
    "    def __init__(self, data, args, save_dir):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            data (_type_): _description_\n",
    "            args (_type_): _description_\n",
    "            save_dir (_type_): _description_\n",
    "            problem_type (str, optional): Either \"GEP\" or \"Benchmark\". Defaults to \"GEP\".\n",
    "            log (bool, optional): _description_. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"X dim: {data.xdim}\")\n",
    "        print(f\"Y dim: {data.ydim}\")\n",
    "\n",
    "        print(f\"Size of mu: {data.nineq}\")\n",
    "        print(f\"Size of lambda: {data.neq}\")\n",
    "\n",
    "        self.data = data\n",
    "        self.args = args\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.outer_iterations = args[\"outer_iterations\"]\n",
    "        self.inner_iterations = args[\"inner_iterations\"]\n",
    "        self.tau = args[\"tau\"]\n",
    "        self.rho = args[\"rho\"]\n",
    "        self.rho_max = args[\"rho_max\"]\n",
    "        self.alpha = args[\"alpha\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.hidden_sizes = args[\"hidden_sizes\"]\n",
    "\n",
    "        self.primal_lr = args[\"primal_lr\"]\n",
    "        self.dual_lr = args[\"dual_lr\"]\n",
    "        self.decay = args[\"decay\"]\n",
    "        self.patience = args[\"patience\"]\n",
    "        \n",
    "        # for logging\n",
    "        self.step = 0\n",
    "\n",
    "        X = data.X\n",
    "        eq_cm = data.eq_cm\n",
    "        ineq_cm = data.ineq_cm\n",
    "        eq_rhs = data.eq_rhs\n",
    "        ineq_rhs = data.ineq_rhs\n",
    "\n",
    "        train = data.train_indices\n",
    "        valid = data.valid_indices\n",
    "        test = data.test_indices\n",
    "\n",
    "        # Traning data in a data set\n",
    "        #! Vary per experiment\n",
    "        # self.train_dataset = CustomDataset(X[train].to(DEVICE), eq_cm[train], ineq_cm[train], eq_rhs[train], ineq_rhs[train])\n",
    "        self.train_dataset = CustomDataset(X[train].to(DEVICE), eq_cm, ineq_cm, eq_rhs[train], ineq_rhs)\n",
    "        self.valid_dataset = CustomDataset(X[valid].to(DEVICE), eq_cm, ineq_cm, eq_rhs[valid], ineq_rhs)\n",
    "        self.test_dataset = CustomDataset(X[test].to(DEVICE), eq_cm, ineq_cm, eq_rhs[test], ineq_rhs)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, batch_size=1000, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "        self.primal_net = PrimalNet(self.data, self.hidden_sizes).to(dtype=DTYPE, device=DEVICE)\n",
    "        self.dual_net = DualNet(self.data, self.hidden_sizes, self.data.nineq, self.data.neq).to(dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "        self.primal_optim = torch.optim.Adam(self.primal_net.parameters(), lr=self.primal_lr)\n",
    "        self.dual_optim = torch.optim.Adam(self.dual_net.parameters(), lr=self.dual_lr)\n",
    "\n",
    "        # Add schedulers\n",
    "        self.primal_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.primal_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "        self.dual_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.dual_optim, mode='min', factor=self.decay, patience=self.patience\n",
    "        )\n",
    "\n",
    "    def train_PDL(self,):\n",
    "        try:\n",
    "            prev_v_k = 0\n",
    "            for k in range(self.outer_iterations):\n",
    "                begin_time = time.time()\n",
    "                epoch_stats = {}\n",
    "                frozen_dual_net = copy.deepcopy(self.dual_net)\n",
    "                # self.logger.log_rho_vk(self.rho, prev_v_k, self.step)\n",
    "                for l1 in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update primal net using primal loss\n",
    "                    self.primal_net.train()\n",
    "\n",
    "                    # Accumulate training loss over all batches\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        self.primal_optim.zero_grad()\n",
    "                        y = self.primal_net(Xtrain, Xtrain, self.train_dataset.ineq_rhs)\n",
    "                        with torch.no_grad():\n",
    "                            mu, lamb = frozen_dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                        batch_loss = self.primal_loss(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, Xtrain, self.train_dataset.ineq_rhs, mu, lamb).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.primal_optim.step()\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    with torch.no_grad():\n",
    "                        self.primal_net.eval()\n",
    "                        frozen_dual_net.eval()\n",
    "                        val_loss = 0\n",
    "                        for Xvalid in self.valid_loader:\n",
    "                            # for Xvalid, valid_eq_cm, valid_ineq_cm, valid_eq_rhs, valid_ineq_rhs in self.valid_loader:\n",
    "                            y = self.primal_net(Xvalid, Xvalid, self.valid_dataset.ineq_rhs)\n",
    "                            mu, lamb = frozen_dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            val_loss += self.primal_loss(y, self.valid_dataset.eq_cm, self.valid_dataset.ineq_cm, Xvalid, self.valid_dataset.ineq_rhs, mu, lamb).sum()\n",
    "                        val_loss /= len(self.valid_loader)\n",
    "                        # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                        self.primal_scheduler.step(torch.sign(val_loss) * (torch.abs(val_loss) / self.rho))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Copy primal net into frozen primal net\n",
    "                    frozen_primal_net = copy.deepcopy(self.primal_net)\n",
    "\n",
    "                    # Calculate v_k\n",
    "                    y = frozen_primal_net(self.train_dataset.x, self.train_dataset.eq_rhs, self.train_dataset.ineq_rhs)\n",
    "                    mu_k, lamb_k = frozen_dual_net(self.train_dataset.x, self.train_dataset.eq_cm)\n",
    "                    v_k = self.violation(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, self.train_dataset.eq_rhs, self.train_dataset.ineq_rhs, mu_k)\n",
    "\n",
    "                for l in range(self.inner_iterations):\n",
    "                    self.step += 1\n",
    "                    # Update dual net using dual loss\n",
    "                    self.dual_net.train()\n",
    "                    frozen_primal_net.train()\n",
    "                    for Xtrain in self.train_loader:\n",
    "                        self.dual_optim.zero_grad()\n",
    "                        mu, lamb = self.dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                        with torch.no_grad():\n",
    "                            mu_k, lamb_k = frozen_dual_net(Xtrain, self.train_dataset.eq_cm)\n",
    "                            y = frozen_primal_net(Xtrain, Xtrain, self.train_dataset.ineq_rhs)\n",
    "                        # ! Test other loss!\n",
    "                        batch_loss = self.dual_loss(y, self.train_dataset.eq_cm, self.train_dataset.ineq_cm, Xtrain, self.train_dataset.ineq_rhs, mu, lamb, mu_k, lamb_k).mean()\n",
    "                        # batch_loss = self.dual_loss_changed(y, train_eq_cm, train_ineq_cm, train_eq_rhs, train_ineq_rhs, mu, lamb, mu_k, lamb_k).mean()\n",
    "                        batch_loss.backward()\n",
    "                        self.dual_optim.step()\n",
    "                    \n",
    "                    # with torch.no_grad():\n",
    "                    #     # Logg training loss:\n",
    "                    #     self.logger.log_loss(batch_loss, \"dual\", self.step)\n",
    "                    #     self.logger.log_train(self.data, primal_net=frozen_primal_net, dual_net=self.dual_net, step=self.step)\n",
    "\n",
    "                    # Evaluate validation loss every epoch, and update learning rate\n",
    "                    # TODO! Does scheduler correctly decrease LR when rho is increased, if the training set is small?\n",
    "                    with torch.no_grad():\n",
    "                        frozen_primal_net.eval()\n",
    "                        self.dual_net.eval()\n",
    "                        val_loss = 0\n",
    "                        for Xvalid in self.valid_loader:\n",
    "                            y = frozen_primal_net(Xvalid, Xvalid, self.valid_dataset.ineq_rhs)\n",
    "                            mu_valid, lamb_valid = self.dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            mu_k_valid, lamb_k_valid = frozen_dual_net(Xvalid, self.valid_dataset.eq_cm)\n",
    "                            val_loss += self.dual_loss(y, self.valid_dataset.eq_cm, self.valid_dataset.ineq_cm, Xvalid, self.valid_dataset.ineq_rhs, mu_valid, lamb_valid, mu_k_valid, lamb_k_valid).sum()\n",
    "                        val_loss /= len(self.valid_loader)\n",
    "                    # Normalize by rho, so that the schedular still works correctly if rho is increased\n",
    "                    self.dual_scheduler.step(torch.sign(val_loss) * (torch.abs(val_loss) / self.rho))\n",
    "\n",
    "                end_time = time.time()\n",
    "                stats = epoch_stats\n",
    "                print(\"-\"*40)\n",
    "                print(f\"Epoch {k} done. Time taken: {end_time - begin_time}. Rho: {self.rho}. Primal LR: {self.primal_optim.param_groups[0]['lr']}, Dual LR: {self.dual_optim.param_groups[0]['lr']}\")\n",
    "\n",
    "                # Update rho from the second iteration onward.\n",
    "                if k > 0 and v_k > self.tau * prev_v_k:\n",
    "                    self.rho = np.min([self.alpha * self.rho, self.rho_max])\n",
    "\n",
    "                prev_v_k = v_k\n",
    "            \n",
    "                print(f\"Validation set evaluate:\")\n",
    "                with torch.no_grad():\n",
    "                    self.primal_net.eval()\n",
    "                    self.dual_net.eval()\n",
    "                    self.evaluate(self.valid_loader, self.valid_dataset, self.primal_net, self.dual_net)\n",
    "                    \n",
    "            print(\"-\"*40)\n",
    "            print(f\"Test set evaluate:\")\n",
    "            with torch.no_grad():\n",
    "                self.primal_net.eval()\n",
    "                self.dual_net.eval()\n",
    "                self.evaluate(self.test_loader, self.test_dataset, self.primal_net, self.dual_net)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e, flush=True)\n",
    "            # # Ensure writer is closed even if an exception occurs\n",
    "            # if self.logger:\n",
    "            #     self.logger.close()\n",
    "            raise\n",
    "\n",
    "        with open(os.path.join(self.save_dir, 'stats.dict'), 'wb') as f:\n",
    "            pickle.dump(stats, f)\n",
    "        with open(os.path.join(self.save_dir, 'primal_net.dict'), 'wb') as f:\n",
    "            torch.save(self.primal_net.state_dict(), f)\n",
    "        with open(os.path.join(self.save_dir, 'dual_net.dict'), 'wb') as f:\n",
    "            torch.save(self.dual_net.state_dict(), f)\n",
    "\n",
    "        return self.primal_net, self.dual_net, stats\n",
    "\n",
    "    def evaluate(self, loader, dataset, primal_net, dual_net):        \n",
    "        obj_values = []\n",
    "        primal_losses = []\n",
    "        ineq_max_vals = []\n",
    "        ineq_mean_vals = []\n",
    "        eq_max_vals = []\n",
    "        eq_mean_vals = []\n",
    "\n",
    "        for X in loader:\n",
    "\n",
    "            # Forward pass through networks\n",
    "            Y = primal_net(X, dataset.eq_rhs, dataset.ineq_rhs)\n",
    "            mu, lamb = dual_net(X, dataset.eq_cm)\n",
    "\n",
    "            # Compute and store metrics\n",
    "            obj_values.append(self.data.obj_fn(Y).detach().cpu().numpy())\n",
    "            primal_losses.append(self.primal_loss(Y, dataset.eq_cm, dataset.ineq_cm, X, dataset.ineq_rhs, mu, lamb).detach().cpu().numpy())\n",
    "            ineq_dist = self.data.ineq_dist(Y, dataset.ineq_cm, dataset.ineq_rhs)\n",
    "            eq_resid = self.data.eq_resid(Y, dataset.eq_cm, X)\n",
    "\n",
    "            ineq_max_vals.append(torch.max(ineq_dist, dim=1)[0].detach().cpu().numpy())\n",
    "            ineq_mean_vals.append(torch.mean(ineq_dist, dim=1).detach().cpu().numpy())\n",
    "            eq_max_vals.append(torch.max(torch.abs(eq_resid), dim=1)[0].detach().cpu().numpy())\n",
    "            eq_mean_vals.append(torch.mean(torch.abs(eq_resid), dim=1).detach().cpu().numpy())\n",
    "\n",
    "        # Convert lists to arrays for easier handling\n",
    "        obj_values = np.concatenate(obj_values)\n",
    "        primal_losses = np.concatenate(primal_losses)\n",
    "        ineq_max_vals = np.concatenate(ineq_max_vals)\n",
    "        ineq_mean_vals = np.concatenate(ineq_mean_vals)\n",
    "        eq_max_vals = np.concatenate(eq_max_vals)\n",
    "        eq_mean_vals = np.concatenate(eq_mean_vals)\n",
    "\n",
    "        # Print aggregated statistics\n",
    "        print(f\"Obj: {np.mean(obj_values)}\")\n",
    "        print(f\"Primal Loss: {np.mean(primal_losses)}\")\n",
    "        print(f\"Ineq max: {np.mean(ineq_max_vals)}\")\n",
    "        print(f\"Ineq mean: {np.mean(ineq_mean_vals)}\")\n",
    "        print(f\"Eq max: {np.mean(eq_max_vals)}\")\n",
    "        print(f\"Eq mean: {np.mean(eq_mean_vals)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def primal_loss(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb):\n",
    "        obj = self.data.obj_fn(y)\n",
    "        \n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs)\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)\n",
    "\n",
    "        # ! Clamp mu?\n",
    "        # Element-wise clamping of mu_i when g_i (ineq) is negative\n",
    "        # mu = torch.where(ineq < 0, torch.zeros_like(mu), mu)\n",
    "        # ! Clamp ineq_resid?\n",
    "        # ineq = ineq.clamp(min=0)\n",
    "\n",
    "        lagrange_ineq = torch.sum(mu * ineq, dim=1)  # Shape (batch_size,)\n",
    "\n",
    "        lagrange_eq = torch.sum(lamb * eq, dim=1)   # Shape (batch_size,)\n",
    "\n",
    "        violation_ineq = torch.sum(torch.maximum(ineq, torch.zeros_like(ineq)) ** 2, dim=1)\n",
    "        violation_eq = torch.sum(eq ** 2, dim=1)\n",
    "        penalty = self.rho/2 * (violation_ineq + violation_eq)\n",
    "\n",
    "        loss = (obj + (lagrange_ineq + lagrange_eq + penalty))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def dual_loss(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb, mu_k, lamb_k):\n",
    "        # mu = [batch, g]\n",
    "        # lamb = [batch, h]\n",
    "\n",
    "        # g(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs) # [batch, g]\n",
    "        # h(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)   # [batch, h]\n",
    "\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        target_mu = torch.maximum(mu_k + self.rho * ineq, torch.zeros_like(ineq))\n",
    "        # target_mu = torch.maximum(mu_k + 1e-1 * ineq, torch.zeros_like(ineq))\n",
    "\n",
    "        dual_resid_ineq = mu - target_mu # [batch, g]\n",
    "\n",
    "        dual_resid_ineq = torch.norm(dual_resid_ineq, dim=1)  # [batch]\n",
    "\n",
    "        # Compute the dual residuals for equality constraints\n",
    "        #! From 2nd PDL paper, fix to 1e-1, not rho\n",
    "        dual_resid_eq = lamb - (lamb_k + self.rho * eq)\n",
    "        # dual_resid_eq = lamb - (lamb_k + 1e-1 * eq)\n",
    "        dual_resid_eq = torch.norm(dual_resid_eq, dim=1)  # Norm along constraint dimension\n",
    "\n",
    "        loss = (dual_resid_ineq + dual_resid_eq)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def dual_loss_changed(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu, lamb, mu_k, lamb_k):\n",
    "        #! We maximize the dual obj func, so to use it in the loss, take the negation.\n",
    "        dual_obj = -self.data.dual_obj_fn(eq_rhs, ineq_rhs, mu, lamb)\n",
    "\n",
    "        #! Enforced with ReLU.\n",
    "        # ineq = self.data.dual_ineq_resid(mu, lamb)\n",
    "\n",
    "        eq = self.data.dual_eq_resid(mu, lamb, eq_cm, ineq_cm)\n",
    "        # Lagrange multiplier becomes y\n",
    "        lagrange_eq = torch.sum(y * eq, dim=1)\n",
    "\n",
    "        violation_eq = torch.sum(eq ** 2, dim=1)\n",
    "\n",
    "        penalty = self.rho/2 * violation_eq\n",
    "\n",
    "        loss = dual_obj + lagrange_eq + penalty\n",
    "        # loss = dual_obj + penalty\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def violation(self, y, eq_cm, ineq_cm, eq_rhs, ineq_rhs, mu_k):\n",
    "        # Calculate the equality constraint function h_x(y)\n",
    "        eq = self.data.eq_resid(y, eq_cm, eq_rhs)  # Assume shape (num_samples, n_eq)\n",
    "        \n",
    "        # Calculate the infinity norm of h_x(y)\n",
    "        eq_inf_norm = torch.abs(eq).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Calculate the inequality constraint function g_x(y)\n",
    "        ineq = self.data.ineq_resid(y, ineq_cm, ineq_rhs)  # Assume shape (num_samples, n_ineq)\n",
    "        \n",
    "        # Calculate sigma_x(y) for each inequality constraint\n",
    "        sigma_y = torch.maximum(ineq, -mu_k / self.rho)  # Element-wise max\n",
    "        \n",
    "        # Calculate the infinity norm of sigma_x(y)\n",
    "        sigma_y_inf_norm = torch.abs(sigma_y).max(dim=1).values  # Shape: (num_samples,)\n",
    "\n",
    "        # Compute v_k as the maximum of the two norms\n",
    "        v_k = torch.maximum(eq_inf_norm, sigma_y_inf_norm)  # Shape: (num_samples,)\n",
    "        \n",
    "        return v_k.max().item()\n",
    "\n",
    "class PrimalNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        \n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes + [data.ydim]\n",
    "        layers = []\n",
    "\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            if out_size != data.ydim:  # Add ReLU activation for hidden layers only\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, eq_rhs, ineq_rhs):\n",
    "        return self.net(x)\n",
    "\n",
    "class DualNet(nn.Module):\n",
    "    def __init__(self, data, hidden_sizes, mu_size, lamb_size):\n",
    "        super().__init__()\n",
    "        self._data = data\n",
    "        self._hidden_sizes = hidden_sizes\n",
    "        self._mu_size = mu_size\n",
    "        self._lamb_size = lamb_size\n",
    "\n",
    "        # Create the list of layer sizes\n",
    "        layer_sizes = [data.xdim] + self._hidden_sizes\n",
    "        # layer_sizes = [2*data.xdim + 1000] + self._hidden_sizes\n",
    "        layers = []\n",
    "        # Create layers dynamically based on the provided hidden_sizes\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Initialize all layers\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        # Add the output layer\n",
    "        self.out_layer = nn.Linear(self._hidden_sizes[-1], self._mu_size + self._lamb_size)\n",
    "        nn.init.zeros_(self.out_layer.weight)  # Initialize output layer weights to 0\n",
    "        nn.init.zeros_(self.out_layer.bias)    # Initialize output layer biases to 0\n",
    "        layers.append(self.out_layer)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, *args):\n",
    "        out = self.net(x)\n",
    "        #! ReLU to enforce nonnegativity in mu. Test with it.\n",
    "        #! Does this work with zero initialization?\n",
    "        # out_mu = torch.relu(out[:, :self._mu_size])\n",
    "        out_mu = out[:, :self._mu_size]\n",
    "        out_lamb = out[:, self._mu_size:]\n",
    "        return out_mu, out_lamb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running osqp\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "num_var = 100\n",
    "num_ineq = 50\n",
    "num_eq = 50\n",
    "num_examples = 10000\n",
    "\n",
    "save_dir = \"benchmark_experiment_output\"\n",
    "\n",
    "data = create_QP_dataset(num_var, num_ineq, num_eq, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dim: 50\n",
      "Y dim: 100\n",
      "Size of mu: 50\n",
      "Size of lambda: 50\n",
      "----------------------------------------\n",
      "Epoch 0 done. Time taken: 733.6000733375549. Rho: 0.5. Primal LR: 0.0001, Dual LR: 6.491026283684025e-05\n",
      "Validation set evaluate:\n",
      "Obj: -16.738317089384445\n",
      "Primal Loss: -14.247675130564444\n",
      "Ineq max: 0.4433426452503992\n",
      "Ineq mean: 0.04646640769533794\n",
      "Eq max: 0.8352204685184776\n",
      "Eq mean: 0.1757246383265363\n",
      "----------------------------------------\n",
      "Epoch 1 done. Time taken: 694.607549905777. Rho: 0.5. Primal LR: 6.361854860638712e-05, Dual LR: 4.255901233886549e-05\n",
      "Validation set evaluate:\n",
      "Obj: -15.01993373317896\n",
      "Primal Loss: -14.908558875539267\n",
      "Ineq max: 0.04429696080577451\n",
      "Ineq mean: 0.0033768584008977746\n",
      "Eq max: 0.07430474929410694\n",
      "Eq mean: 0.022267517506934432\n",
      "----------------------------------------\n",
      "Epoch 2 done. Time taken: 708.927197933197. Rho: 0.5. Primal LR: 4.04731972678324e-05, Dual LR: 2.8186069554046354e-05\n",
      "Validation set evaluate:\n",
      "Obj: -15.093072320725486\n",
      "Primal Loss: -14.887083946311634\n",
      "Ineq max: 0.11940013497342644\n",
      "Ineq mean: 0.010926466282070202\n",
      "Eq max: 0.041119795692565075\n",
      "Eq mean: 0.01290673378462585\n",
      "----------------------------------------\n",
      "Epoch 3 done. Time taken: 687.6406829357147. Rho: 5.0. Primal LR: 2.549097606963093e-05, Dual LR: 1.885568451673771e-05\n",
      "Validation set evaluate:\n",
      "Obj: -14.965144110289962\n",
      "Primal Loss: -14.949747714912675\n",
      "Ineq max: 0.014039910152122637\n",
      "Ineq mean: 0.0007729753785135099\n",
      "Eq max: 0.010292623553436776\n",
      "Eq mean: 0.0031665012219288836\n",
      "----------------------------------------\n",
      "Epoch 4 done. Time taken: 685.9514961242676. Rho: 5.0. Primal LR: 1.6216989001100652e-05, Dual LR: 1.199571281934779e-05\n",
      "Validation set evaluate:\n",
      "Obj: -14.949979788996622\n",
      "Primal Loss: -14.778849886919497\n",
      "Ineq max: 0.03205455470140036\n",
      "Ineq mean: 0.002404054006184137\n",
      "Eq max: 0.010632827377184243\n",
      "Eq mean: 0.0031338964098128665\n",
      "----------------------------------------\n",
      "Epoch 5 done. Time taken: 758.9053120613098. Rho: 50.0. Primal LR: 1.0213842899856094e-05, Dual LR: 8.105851616218128e-06\n",
      "Validation set evaluate:\n",
      "Obj: -14.851253725485504\n",
      "Primal Loss: -14.743935797448499\n",
      "Ineq max: 0.009741794010851056\n",
      "Ineq mean: 0.00039479842618908855\n",
      "Eq max: 0.006046023586497709\n",
      "Eq mean: 0.0018934938673663075\n",
      "----------------------------------------\n",
      "Epoch 6 done. Time taken: 1225.8140201568604. Rho: 50.0. Primal LR: 6.497898609824964e-06, Dual LR: 5.156825150425342e-06\n",
      "Validation set evaluate:\n",
      "Obj: -15.033273256137228\n",
      "Primal Loss: -13.969962624929284\n",
      "Ineq max: 0.008441495398740357\n",
      "Ineq mean: 0.0006627661199652131\n",
      "Eq max: 0.006152440898054212\n",
      "Eq mean: 0.0019283313585176309\n",
      "----------------------------------------\n",
      "Epoch 7 done. Time taken: 1062.5618770122528. Rho: 500.0. Primal LR: 4.0925300976303945e-06, Dual LR: 3.6275567655825147e-06\n",
      "Validation set evaluate:\n",
      "Obj: -14.480023955628669\n",
      "Primal Loss: -13.460324289856839\n",
      "Ineq max: 0.005882118951694579\n",
      "Ineq mean: 0.00022532724006115153\n",
      "Eq max: 0.0061406510648746694\n",
      "Eq mean: 0.0019482862876246414\n",
      "----------------------------------------\n",
      "Epoch 8 done. Time taken: 4132.724888086319. Rho: 500.0. Primal LR: 2.6036082493920133e-06, Dual LR: 2.710409185847038e-06\n",
      "Validation set evaluate:\n",
      "Obj: -14.713909787232318\n",
      "Primal Loss: -6.8277881321142\n",
      "Ineq max: 0.0047600086604621525\n",
      "Ineq mean: 0.0003698849926767467\n",
      "Eq max: 0.005974536851840557\n",
      "Eq mean: 0.0018921854732962076\n",
      "----------------------------------------\n",
      "Epoch 9 done. Time taken: 726.3100547790527. Rho: 5000.0. Primal LR: 1.6398140018627685e-06, Dual LR: 2.0871320158888426e-06\n",
      "Validation set evaluate:\n",
      "Obj: -14.57909742883544\n",
      "Primal Loss: -7.309370688410114\n",
      "Ineq max: 0.0017512050375726798\n",
      "Ineq mean: 9.452569652827511e-05\n",
      "Eq max: 0.0056766939940085465\n",
      "Eq mean: 0.0017567104939908312\n",
      "----------------------------------------\n",
      "Test set evaluate:\n",
      "Obj: -14.55828045118212\n",
      "Primal Loss: -8.966900390121001\n",
      "Ineq max: 0.0017218066052583669\n",
      "Ineq mean: 8.270137617736194e-05\n",
      "Eq max: 0.0054880349837705105\n",
      "Eq mean: 0.0016976369227093833\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"outer_iterations\": 10,\n",
    "    \"inner_iterations\": 500,\n",
    "    \"tau\": 0.8,\n",
    "    \"rho\": 0.5,\n",
    "    \"rho_max\": 5000,\n",
    "    \"alpha\": 10,\n",
    "    \"batch_size\": 100,\n",
    "    \"hidden_sizes\": [500, 500],\n",
    "    \"primal_lr\": 1e-4,\n",
    "    \"dual_lr\": 1e-4,\n",
    "    \"decay\": 0.99,\n",
    "    \"patience\": 10,\n",
    "    \"corrEps\": 1e-4,\n",
    "    \"train\": 0.8,\n",
    "    \"valid\": 0.1,\n",
    "    \"test\": 0.1\n",
    "}\n",
    "\n",
    "trainer = PrimalDualTrainer(data, args, save_dir)\n",
    "# with profiler.profile(record_shapes=True) as prof:\n",
    "primal_net, dual_net, stats = trainer.train_PDL()\n",
    "    \n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
